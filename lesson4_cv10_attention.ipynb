{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5_cv10_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekity1002/DL4US_homeworks/blob/master/lesson4_cv10_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "65aXp28U_aLO"
      },
      "source": [
        "# Lesson4 ニューラル翻訳モデルを作ってみよう（Seq2Seq, Attention）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wn0-X8r5_aLQ"
      },
      "source": [
        "## Homework\n",
        "\n",
        "RNNを用いて高精度な英日翻訳器を実装してみましょう。\n",
        "\n",
        "ネットワークの形などは特に制限を設けませんし、今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません。\n",
        "\n",
        "精度上位者はリーダーボードに掲載させていただきます。（精度の評価はBLEUスコアによって行います。）\n",
        "\n",
        "## 目標値\n",
        "\n",
        "BLEU: 0.15\n",
        "\n",
        "## ルール\n",
        "\n",
        "- 以下のサンプルも参考にしながら翻訳文を生成しcsvファイルに出力して下さい。\n",
        "- BLEUスコア(4-gramまで)で評価します。\n",
        "- 学習データとテストデータの入力の系列長はpaddingで揃えてあります。\n",
        "\n",
        "## 評価について\n",
        "\n",
        "- テストデータ(x_test)に対する予測ラベルをcsvファイルで提出してください。\n",
        "- ファイル名はsubmission.csvとしてください。\n",
        "- 予測ラベルのy_testに対する精度 (BLEU)で評価します。\n",
        "- 毎日24時にテストデータの一部に対する精度でLeader Boardを更新します。\n",
        "- 最終的な評価はテストデータ全体に対する精度でおこないます。\n",
        "\n",
        "## サンプルコード\n",
        "\n",
        "次のセルで指定されているx_train, y_trainのみを使って学習させてください。　"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uv4LmE7qjkx",
        "colab_type": "code",
        "outputId": "2131c043-2a00-4167-a2bc-68da62002d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vFS1DuA0_aLR",
        "outputId": "76024b6f-52a7-484c-ca5c-e880be810bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import csv\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import add, average, Activation, Bidirectional, CuDNNLSTM, concatenate, Dense, dot, Embedding, Input, LSTM\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from functools import partial\n",
        "np.load = partial(np.load, allow_pickle=True)  # monkey patch\n",
        "\n",
        "def load_data():\n",
        "    # 学習データ\n",
        "    # x_train = np.load('/root/userspace/lesson4/data/x_train.npy')\n",
        "    # y_train = np.load('/root/userspace/lesson4/data/y_train.npy')\n",
        "    # tokenizer_en = np.load('/root/userspace/lesson4/data/tokenizer_en.npy').item()\n",
        "    # tokenizer_ja = np.load('/root/userspace/lesson4/data/tokenizer_ja.npy').item()\n",
        "\n",
        "    x_train = np.load('/content/drive/My Drive/Colab Notebooks/x_train.npy')\n",
        "    y_train = np.load('/content/drive/My Drive/Colab Notebooks/y_train.npy')\n",
        "    tokenizer_en = np.load('/content/drive/My Drive/Colab Notebooks/tokenizer_en.npy').item()\n",
        "    tokenizer_ja = np.load('/content/drive/My Drive/Colab Notebooks/tokenizer_ja.npy').item()\n",
        "\n",
        "    # テストデータ\n",
        "    x_test = np.load('/content/drive/My Drive/Colab Notebooks/x_test.npy')\n",
        "\n",
        "    return (x_train, y_train, tokenizer_en, tokenizer_ja, x_test)\n",
        "\n",
        "x_train, y_train, tokenizer_en, tokenizer_ja, x_test = load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "95CcJbT2_aLT",
        "colab": {}
      },
      "source": [
        "emb_dim = 1024\n",
        "hid_dim = 1024\n",
        "att_dim = 1024\n",
        "\n",
        "# LSTM mask_zero オプション使うので +1 する\n",
        "en_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "ja_vocab_size = len(tokenizer_ja.word_index) + 1\n",
        "\n",
        "seqX_len = len(x_train[0])\n",
        "seqY_len = len(y_train[0])\n",
        "\n",
        "encoder_states = []\n",
        "\n",
        "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
        "bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N9kDInGtEmqr",
        "outputId": "f9468b59-7294-4d24-d0d3-36b9ee78a751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "def build_attention_models():\n",
        "    # 訓練用モデル\n",
        "    # 符号化器\n",
        "    layer_num = 2\n",
        "    encoder_inputs = Input(shape=(seqX_len,))\n",
        "    encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs)\n",
        "\n",
        "#     encoder_states = []\n",
        "#     encoded_seq, *states = LSTM(hid_dim, return_sequences=True, return_state=True)(encoder_embedded)\n",
        "#     encoder_states.append(states)\n",
        "#     encoded_seq, *states = LSTM(hid_dim, return_sequences=True, return_state=True)(encoded_seq)\n",
        "#     encoder_states.append(states)\n",
        "    encoded_seq, *encoder_states = LSTM(hid_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)(encoder_embedded)\n",
        "    print(encoder_states)\n",
        "    \n",
        "    # 復号化器（encoder_statesを初期状態として指定）\n",
        "    decoder_inputs = Input(shape=(seqY_len,))\n",
        "    decoder_embedding = Embedding(ja_vocab_size, emb_dim)\n",
        "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "#     decoder_lstm_1 = CuDNNLSTM(hid_dim, return_sequences=True, return_state=True)\n",
        "#     decoder_lstm_2 = CuDNNLSTM(hid_dim, return_sequences=True, return_state=True)\n",
        "    decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True,dropout=0.2, recurrent_dropout=0.2)\n",
        "#     decoded_seq, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "#     for i, s in enumerate(encoder_states):\n",
        "#         if i == 0:\n",
        "#             decoded_seq, _, _ = decoder_lstm_1(decoder_embedded, initial_state=s)\n",
        "#         else:\n",
        "#             decoded_seq, _, _ = decoder_lstm_2(decoded_seq, initial_state=s)\n",
        "    decoded_seq, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "\n",
        "    # Attention\n",
        "    print('decoded_seq: ', decoded_seq)\n",
        "    score_dense = Dense(hid_dim)\n",
        "    score = score_dense(decoded_seq) # shape: (seqY_len, hid_dim) -> (seqY_len, hid_dim)\n",
        "    print('score, encoded_seq: ',score, encoded_seq)\n",
        "    score = dot([score, encoded_seq], axes=(2,2))           # shape: [(seqY_len, hid_dim), (seqX_len, hid_dim)] -> (seqY_len, seqX_len)\n",
        "    #print('dot score: ',score)\n",
        "    attention = Activation('softmax')(score)                # shape: (seqY_len, seqX_len) -> (seqY_len, seqX_len)\n",
        "    #print('attention', attention)\n",
        "    context = dot([attention, encoded_seq], axes=(2,1))     # shape: [(seqY_len, seqX_len), (seqX_len, hid_dim)] -> (seqY_len, hid_dim)\n",
        "    #print('context', context)\n",
        "    concat = concatenate([context, decoded_seq], axis=2)    # shape: [(seqY_len, hid_dim), (seqY_len, hid_dim)] -> (seqY_len, 2*hid_dim)\n",
        "    attention_dense = Dense(att_dim, activation='tanh')\n",
        "    attentional = attention_dense(concat)                   # shape: (seqY_len, 2*hid_dim) -> (seqY_len, att_dim)\n",
        "    output_dense = Dense(ja_vocab_size, activation='softmax')\n",
        "    outputs = output_dense(attentional)                     # shape: (seqY_len, att_dim) -> (seqY_len, ja_vocab_size)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "    # -----------------------------------\n",
        "    # 生成用モデル\n",
        "    encoder_model = Model(encoder_inputs, [encoded_seq]+encoder_states)\n",
        "    #encoder_model = Model(encoder_inputs, [encoded_seq]+encoder_states[-1])\n",
        "\n",
        "    #decoder_states_inputs1 = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
        "    #decoder_states_inputs2 = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
        "    decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
        "    decoder_inputs = Input(shape=(1,))\n",
        "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "#     for i, s in enumerate(encoder_states):\n",
        "#         if i == 0:\n",
        "#             decoder_seq, *decoder_states = decoder_lstm_1(decoder_embedded, initial_state=decoder_states_inputs1)\n",
        "#         else:\n",
        "#             decoder_seq, *decoder_states = decoder_lstm_2(decoder_seq, initial_state=decoder_states_inputs2)\n",
        "    decoder_seq, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
        "    print(decoder_seq)\n",
        "    print(decoder_states)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_seq] + decoder_states)\n",
        "\n",
        "    # Attention\n",
        "    encoded_seq_in, decoded_seq_in = Input(shape=(seqX_len, hid_dim)), Input(shape=(1, hid_dim))\n",
        "    score =score_dense(decoded_seq_in)\n",
        "    score = dot([score, encoded_seq_in], axes=(2,2))\n",
        "    attention = Activation('softmax')(score)\n",
        "    context = dot([attention, encoded_seq_in], axes=(2,1))\n",
        "    concat = concatenate([context, decoded_seq_in], axis=2)\n",
        "    attentional = attention_dense(concat)\n",
        "    attention_outputs = output_dense(attentional)\n",
        "\n",
        "    attention_model = Model([encoded_seq_in, decoded_seq_in], [attention_outputs, attention])\n",
        "    return model, encoder_model, decoder_model, attention_model\n",
        "_,_,_,_ = build_attention_models()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "[<tf.Tensor 'lstm/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_1/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_1_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_1_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_1_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ehTFDZtEE1J3",
        "colab": {}
      },
      "source": [
        "def plot_acc_and_loss(history):\n",
        "    \"\"\"plot history\"\"\"\n",
        "    fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
        "    \n",
        "#     acc = history.history['acc']\n",
        "#     val_acc = history.history['val_acc']    \n",
        "#     epochs=range(1, len(acc)+1)    \n",
        "#     ax[0].plot(epochs, acc, label='Train')\n",
        "#     ax[0].plot(epochs, val_acc, label='Val')\n",
        "#     ax[0].legend()\n",
        "#     ax[0].set_title('Accuracy')\n",
        "    \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs=range(1, len(loss)+1)\n",
        "    ax[1].plot(epochs, loss, label='Train')\n",
        "    ax[1].plot(epochs, val_loss, label='Val')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_title('Loss')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qw45VigRE8Wg",
        "outputId": "4b015610-3dc9-48bd-9331-757b5c0183a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "epochs=1\n",
        "batch_size=512\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10),\n",
        "    #ModelCheckpoint(filepath=modelpath, monitor='val_acc', save_best_only=True)\n",
        "]\n",
        "train_model, encoder_model, decoder_model, attention_model = build_attention_models()\n",
        "history = train_model.fit([x_train, y_train], np.expand_dims(train_target, -1), \n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2, \n",
        "                    callbacks=callbacks\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_3/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_3_1/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_2/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_3_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_3_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_3_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 39200 samples, validate on 9800 samples\n",
            "39200/39200 [==============================] - 120s 3ms/sample - loss: 3.7074 - val_loss: 2.5856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PVXQtfz9NJ56",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq, bos_eos, max_output_length,\n",
        "                    encoder_model, decoder_model, attention_model):\n",
        "    \"\"\"\n",
        "    input_seq : 入力する文 array-like, shape=(1, seq_len)\n",
        "    bos_eos : 文の最初と最後を表す特殊文字のインデックス list\n",
        "    max_output_length : 最大出力語数\n",
        "    \"\"\"\n",
        "    #states_value = encoder_model.predict(input_seq)\n",
        "    encoded_seq, *states_value = encoder_model.predict(input_seq)\n",
        "    #print(encoded_seq)\n",
        "    #print(states_value)\n",
        "    target_seq = np.array(bos_eos[0])\n",
        "    output_seq = bos_eos[0][:]\n",
        "    attention_seq = np.empty((0, len(input_seq[0]))) #初期化せずに配列生成\n",
        "    #print(\"seqs:\", target_seq, output_seq, attention_seq)\n",
        "\n",
        "    while True:\n",
        "        decoded_seq, *states_value = decoder_model.predict([target_seq] + states_value) #開始を表す一文字と encoder_model の状態のリスト\n",
        "        output_tokens, attention = attention_model.predict([encoded_seq, decoded_seq])\n",
        "        print(\"---------\")\n",
        "        print(np.array(output_tokens).shape, output_tokens)\n",
        "        sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
        "        output_seq += sampled_token_index\n",
        "        attention_seq = np.append(attention_seq, attention[0], axis=0)\n",
        "        \n",
        "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
        "            break\n",
        "\n",
        "        target_seq = np.array(sampled_token_index)\n",
        "\n",
        "    return output_seq, attention_seq\n",
        "\n",
        "def mean_bleu(x_train, y_train, encoder_model, decoder_model, attention_model,\n",
        "              start_i=0, end_i=None):\n",
        "    \"\"\"データでBELUの平均値確認\"\"\"\n",
        "    out_seqs, att_seqs = [], []\n",
        "    bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
        "    if end_i is None:\n",
        "        end_i = len(x_train)\n",
        "\n",
        "    for i in range(len(x_train[:end_i])):\n",
        "        out, att = decode_sequence(x_train[i][np.newaxis,:], bos_eos, 100,\n",
        "                                   encoder_model, decoder_model, attention_model)\n",
        "        out_seqs.append(out)\n",
        "        att_seqs.append(att)\n",
        "    tr_output = [out_seqs[i][1:-1] for i in range(len(x_train[:end_i]))]    \n",
        "\n",
        "    pred_output = [[tokenizer_ja.index_word[idx] for idx in seq if idx != 0] for seq in tr_output]\n",
        "    true_output = [[tokenizer_ja.index_word[idx] for idx in seq if idx != 0] for seq in y_train]\n",
        "    bleu_scores = [sentence_bleu([true[1:-1]], pred) for true, pred in zip(true_output, pred_output)]\n",
        "\n",
        "    print(\"Mean BLEU: \", np.mean(bleu_scores))\n",
        "\n",
        "    return pred_output, true_output, bleu_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EiSJ1TvGsT-Y",
        "outputId": "1f430820-62a2-4cac-a15a-49fbcaae953c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "_, _, _ = mean_bleu(x_train, y_train, encoder_model, decoder_model, attention_model, start_i=0, end_i=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------\n",
            "(1, 1, 8777) [[[2.8475255e-05 3.6483220e-07 3.8614085e-05 ... 4.1695339e-06\n",
            "   5.1268654e-07 1.0325260e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[5.5915542e-04 1.4668708e-09 3.7480233e-05 ... 3.7789775e-08\n",
            "   1.2116634e-09 6.9678023e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.1767040e-06 1.4122185e-06 1.0480249e-05 ... 1.0595589e-05\n",
            "   1.8599945e-06 3.1049542e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.9614958e-04 1.3407708e-09 2.4707962e-05 ... 3.2808437e-08\n",
            "   9.9559727e-10 6.2034462e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[2.1115454e-06 2.8455624e-06 1.0275919e-05 ... 1.6357710e-05\n",
            "   3.9208103e-06 5.6678000e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0173464e-04 1.8034043e-09 2.5420295e-05 ... 4.2256229e-08\n",
            "   1.3417391e-09 8.4188425e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[2.6684816e-06 3.5486041e-06 1.2104732e-05 ... 1.9054534e-05\n",
            "   5.0599633e-06 7.0627693e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.9748143e-04 2.0554729e-09 2.4133853e-05 ... 4.7358501e-08\n",
            "   1.5522479e-09 9.7137800e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.2902797e-06 3.7521779e-06 1.2810834e-05 ... 1.9991216e-05\n",
            "   5.4755665e-06 7.5872663e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.8444562e-04 2.1644884e-09 2.2354725e-05 ... 4.9583658e-08\n",
            "   1.6553534e-09 1.0302136e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.7171033e-06 3.7686993e-06 1.2916039e-05 ... 2.0181351e-05\n",
            "   5.5790370e-06 7.7265368e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.7155690e-04 2.2137607e-09 2.1086900e-05 ... 5.0562964e-08\n",
            "   1.7082489e-09 1.0575170e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.9641927e-06 3.7325406e-06 1.2892162e-05 ... 2.0104575e-05\n",
            "   5.5728146e-06 7.7259820e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.61745886e-04 2.23704855e-09 2.03305244e-05 ... 5.09850580e-08\n",
            "   1.73650228e-09 1.07059615e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0956297e-06 3.6898073e-06 1.2863031e-05 ... 1.9961926e-05\n",
            "   5.5366118e-06 7.6853585e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.5496199e-04 2.2480342e-09 1.9900552e-05 ... 5.1151652e-08\n",
            "   1.7516659e-09 1.0768020e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.16117427e-06 3.65425262e-06 1.28423535e-05 ... 1.98281996e-05\n",
            "   5.49925471e-06 7.64131164e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.5047712e-04 2.2533730e-09 1.9658903e-05 ... 5.1211465e-08\n",
            "   1.7600308e-09 1.0797691e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.1921890e-06 3.6282827e-06 1.2828453e-05 ... 1.9724628e-05\n",
            "   5.4694128e-06 7.6053630e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4755404e-04 2.2556490e-09 1.9520719e-05 ... 5.1220866e-08\n",
            "   1.7644010e-09 1.0809895e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2057709e-06 3.6105089e-06 1.2818482e-05 ... 1.9651152e-05\n",
            "   5.4479324e-06 7.5790344e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4566192e-04 2.2564459e-09 1.9440227e-05 ... 5.1210918e-08\n",
            "   1.7665873e-09 1.0813632e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.21090999e-06 3.59878163e-06 1.28108995e-05 ... 1.96014425e-05\n",
            "   5.43327269e-06 7.56076042e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4443810e-04 2.2565416e-09 1.9392241e-05 ... 5.1195414e-08\n",
            "   1.7675956e-09 1.0813429e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2121833e-06 3.5912255e-06 1.2804963e-05 ... 1.9568794e-05\n",
            "   5.4235861e-06 7.5484472e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4364231e-04 2.2563362e-09 1.9362888e-05 ... 5.1179967e-08\n",
            "   1.7679945e-09 1.0811560e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2118313e-06 3.5864246e-06 1.2800303e-05 ... 1.9547750e-05\n",
            "   5.4172970e-06 7.5402827e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4312118e-04 2.2560396e-09 1.9344378e-05 ... 5.1166850e-08\n",
            "   1.7680858e-09 1.0809294e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2109173e-06 3.5834003e-06 1.2796610e-05 ... 1.9534340e-05\n",
            "   5.4132552e-06 7.5349017e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4277578e-04 2.2557340e-09 1.9332316e-05 ... 5.1156352e-08\n",
            "   1.7680351e-09 1.0807066e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2099077e-06 3.5815096e-06 1.2793689e-05 ... 1.9525840e-05\n",
            "   5.4106677e-06 7.5313501e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4254382e-04 2.2554634e-09 1.9324225e-05 ... 5.1148309e-08\n",
            "   1.7679275e-09 1.0805110e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2089878e-06 3.5803273e-06 1.2791393e-05 ... 1.9520501e-05\n",
            "   5.4090106e-06 7.5290072e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4238552e-04 2.2552380e-09 1.9318630e-05 ... 5.1142180e-08\n",
            "   1.7678082e-09 1.0803454e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2082129e-06 3.5795829e-06 1.2789563e-05 ... 1.9517140e-05\n",
            "   5.4079374e-06 7.5274356e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4227574e-04 2.2550537e-09 1.9314657e-05 ... 5.1137711e-08\n",
            "   1.7676909e-09 1.0802138e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2075881e-06 3.5791161e-06 1.2788127e-05 ... 1.9515033e-05\n",
            "   5.4072425e-06 7.5263820e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4219844e-04 2.2549056e-09 1.9311767e-05 ... 5.1134350e-08\n",
            "   1.7675881e-09 1.0801098e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2070856e-06 3.5788221e-06 1.2786979e-05 ... 1.9513702e-05\n",
            "   5.4067827e-06 7.5256562e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4214291e-04 2.2547924e-09 1.9309598e-05 ... 5.1131831e-08\n",
            "   1.7675027e-09 1.0800248e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2066927e-06 3.5786381e-06 1.2786066e-05 ... 1.9512865e-05\n",
            "   5.4064894e-06 7.5251610e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.42102663e-04 2.25470331e-09 1.93080086e-05 ... 5.11300051e-08\n",
            "   1.76743642e-09 1.07996145e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.20638389e-06 3.57851536e-06 1.27853455e-05 ... 1.95123430e-05\n",
            "   5.40628298e-06 7.52480946e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.42073006e-04 2.25463270e-09 1.93067954e-05 ... 5.11286977e-08\n",
            "   1.76737758e-09 1.07990905e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2061401e-06 3.5784408e-06 1.2784775e-05 ... 1.9512012e-05\n",
            "   5.4061447e-06 7.5245594e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4205074e-04 2.2545892e-09 1.9305853e-05 ... 5.1127710e-08\n",
            "   1.7673333e-09 1.0798719e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2059446e-06 3.5783942e-06 1.2784317e-05 ... 1.9511805e-05\n",
            "   5.4060488e-06 7.5243829e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4203424e-04 2.2545437e-09 1.9305149e-05 ... 5.1126925e-08\n",
            "   1.7672979e-09 1.0798398e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.20579545e-06 3.57836575e-06 1.27839585e-05 ... 1.95116772e-05\n",
            "   5.40598012e-06 7.52425103e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4202207e-04 2.2545144e-09 1.9304603e-05 ... 5.1126449e-08\n",
            "   1.7672679e-09 1.0798175e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2056722e-06 3.5783432e-06 1.2783670e-05 ... 1.9511592e-05\n",
            "   5.4059351e-06 7.5241605e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4201241e-04 2.2544910e-09 1.9304167e-05 ... 5.1125980e-08\n",
            "   1.7672465e-09 1.0797981e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2055776e-06 3.5783307e-06 1.2783431e-05 ... 1.9511543e-05\n",
            "   5.4059014e-06 7.5240846e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4200502e-04 2.2544739e-09 1.9303834e-05 ... 5.1125731e-08\n",
            "   1.7672295e-09 1.0797836e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2054999e-06 3.5783262e-06 1.2783257e-05 ... 1.9511508e-05\n",
            "   5.4058792e-06 7.5240323e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4199961e-04 2.2544606e-09 1.9303592e-05 ... 5.1125472e-08\n",
            "   1.7672157e-09 1.0797732e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2054421e-06 3.5783210e-06 1.2783104e-05 ... 1.9511490e-05\n",
            "   5.4058614e-06 7.5239923e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4199512e-04 2.2544504e-09 1.9303376e-05 ... 5.1125340e-08\n",
            "   1.7672044e-09 1.0797642e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2053889e-06 3.5783169e-06 1.2782992e-05 ... 1.9511486e-05\n",
            "   5.4058496e-06 7.5239614e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4199230e-04 2.2544417e-09 1.9303245e-05 ... 5.1125237e-08\n",
            "   1.7671974e-09 1.0797579e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2053557e-06 3.5783160e-06 1.2782903e-05 ... 1.9511481e-05\n",
            "   5.4058378e-06 7.5239386e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198968e-04 2.2544415e-09 1.9303114e-05 ... 5.1125088e-08\n",
            "   1.7671905e-09 1.0797516e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2053230e-06 3.5783153e-06 1.2782828e-05 ... 1.9511475e-05\n",
            "   5.4058319e-06 7.5239154e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198738e-04 2.2544329e-09 1.9303023e-05 ... 5.1125042e-08\n",
            "   1.7671838e-09 1.0797475e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052998e-06 3.5783162e-06 1.2782770e-05 ... 1.9511472e-05\n",
            "   5.4058278e-06 7.5239109e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.41985753e-04 2.25442953e-09 1.93029391e-05 ... 5.11249674e-08\n",
            "   1.76718451e-09 1.07974385e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052816e-06 3.5783141e-06 1.2782714e-05 ... 1.9511470e-05\n",
            "   5.4058196e-06 7.5238995e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198468e-04 2.2544258e-09 1.9302888e-05 ... 5.1124882e-08\n",
            "   1.7671783e-09 1.0797421e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052634e-06 3.5783128e-06 1.2782684e-05 ... 1.9511483e-05\n",
            "   5.4058178e-06 7.5238886e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198392e-04 2.2544238e-09 1.9302834e-05 ... 5.1124935e-08\n",
            "   1.7671768e-09 1.0797391e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052525e-06 3.5783130e-06 1.2782650e-05 ... 1.9511484e-05\n",
            "   5.4058182e-06 7.5238831e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198342e-04 2.2544226e-09 1.9302825e-05 ... 5.1124911e-08\n",
            "   1.7671760e-09 1.0797386e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052411e-06 3.5783175e-06 1.2782628e-05 ... 1.9511488e-05\n",
            "   5.4058137e-06 7.5238768e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198299e-04 2.2544222e-09 1.9302803e-05 ... 5.1124896e-08\n",
            "   1.7671754e-09 1.0797383e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052320e-06 3.5783166e-06 1.2782613e-05 ... 1.9511484e-05\n",
            "   5.4058137e-06 7.5238763e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.41982261e-04 2.25442154e-09 1.93027772e-05 ... 5.11248821e-08\n",
            "   1.76717496e-09 1.07973595e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052270e-06 3.5783155e-06 1.2782597e-05 ... 1.9511488e-05\n",
            "   5.4058119e-06 7.5238740e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198200e-04 2.2544187e-09 1.9302754e-05 ... 5.1124861e-08\n",
            "   1.7671727e-09 1.0797344e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052220e-06 3.5783148e-06 1.2782583e-05 ... 1.9511495e-05\n",
            "   5.4058110e-06 7.5238722e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198188e-04 2.2544211e-09 1.9302739e-05 ... 5.1124875e-08\n",
            "   1.7671713e-09 1.0797358e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052197e-06 3.5783164e-06 1.2782575e-05 ... 1.9511492e-05\n",
            "   5.4058132e-06 7.5238677e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198142e-04 2.2544182e-09 1.9302732e-05 ... 5.1124854e-08\n",
            "   1.7671691e-09 1.0797343e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052175e-06 3.5783141e-06 1.2782568e-05 ... 1.9511490e-05\n",
            "   5.4058096e-06 7.5238709e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198127e-04 2.2544182e-09 1.9302714e-05 ... 5.1124854e-08\n",
            "   1.7671691e-09 1.0797343e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052116e-06 3.5783160e-06 1.2782561e-05 ... 1.9511501e-05\n",
            "   5.4058123e-06 7.5238668e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198104e-04 2.2544133e-09 1.9302708e-05 ... 5.1124839e-08\n",
            "   1.7671686e-09 1.0797340e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052129e-06 3.5783171e-06 1.2782553e-05 ... 1.9511486e-05\n",
            "   5.4058082e-06 7.5238690e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198107e-04 2.2544178e-09 1.9302710e-05 ... 5.1124847e-08\n",
            "   1.7671687e-09 1.0797341e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052084e-06 3.5783171e-06 1.2782553e-05 ... 1.9511495e-05\n",
            "   5.4058082e-06 7.5238622e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198104e-04 2.2544175e-09 1.9302688e-05 ... 5.1124839e-08\n",
            "   1.7671686e-09 1.0797340e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052066e-06 3.5783153e-06 1.2782559e-05 ... 1.9511497e-05\n",
            "   5.4058114e-06 7.5238654e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198098e-04 2.2544129e-09 1.9302686e-05 ... 5.1124786e-08\n",
            "   1.7671683e-09 1.0797338e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052066e-06 3.5783153e-06 1.2782559e-05 ... 1.9511497e-05\n",
            "   5.4058114e-06 7.5238654e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198104e-04 2.2544133e-09 1.9302688e-05 ... 5.1124793e-08\n",
            "   1.7671686e-09 1.0797340e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052066e-06 3.5783153e-06 1.2782547e-05 ... 1.9511497e-05\n",
            "   5.4058114e-06 7.5238654e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198089e-04 2.2544135e-09 1.9302690e-05 ... 5.1124800e-08\n",
            "   1.7671687e-09 1.0797341e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052038e-06 3.5783164e-06 1.2782551e-05 ... 1.9511503e-05\n",
            "   5.4058132e-06 7.5238609e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4198078e-04 2.2544149e-09 1.9302684e-05 ... 5.1124829e-08\n",
            "   1.7671699e-09 1.0797347e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.2052038e-06 3.5783164e-06 1.2782551e-05 ... 1.9511492e-05\n",
            "   5.4058132e-06 7.5238609e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.41980776e-04 2.25441488e-09 1.93026844e-05 ... 5.11248288e-08\n",
            "   1.76716652e-09 1.07973275e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[1.5623469e-05 2.6419056e-07 1.9858251e-05 ... 2.9471814e-06\n",
            "   3.5900356e-07 7.4599541e-07]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[5.6470366e-04 1.1751241e-09 3.2413744e-05 ... 3.0665792e-08\n",
            "   9.4044261e-10 5.8909824e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[2.8954619e-06 1.2751183e-06 8.9953410e-06 ... 9.2651680e-06\n",
            "   1.5997595e-06 2.8912928e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0347895e-04 1.1949538e-09 2.3401219e-05 ... 2.8877292e-08\n",
            "   8.6200069e-10 5.7649805e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[1.9459069e-06 2.6819553e-06 9.4174529e-06 ... 1.4775971e-05\n",
            "   3.5426770e-06 5.4953839e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0441280e-04 1.6316932e-09 2.4248780e-05 ... 3.7397605e-08\n",
            "   1.1836059e-09 7.9450846e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[2.5119327e-06 3.3639831e-06 1.1413565e-05 ... 1.7360839e-05\n",
            "   4.6186046e-06 6.8983386e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.9646123e-04 1.8594860e-09 2.2893546e-05 ... 4.1749416e-08\n",
            "   1.3720880e-09 9.1734922e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.1406687e-06 3.5512699e-06 1.2201165e-05 ... 1.8249219e-05\n",
            "   5.0033004e-06 7.4101836e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.8145832e-04 1.9556989e-09 2.1099917e-05 ... 4.3590497e-08\n",
            "   1.4632260e-09 9.7213659e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.5773551e-06 3.5580870e-06 1.2347872e-05 ... 1.8420960e-05\n",
            "   5.0937456e-06 7.5354442e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.6761479e-04 1.9982471e-09 1.9847419e-05 ... 4.4374357e-08\n",
            "   1.5095937e-09 9.9711333e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.8319904e-06 3.5172368e-06 1.2344573e-05 ... 1.8340603e-05\n",
            "   5.0836693e-06 7.5251519e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.5735595e-04 2.0182858e-09 1.9111174e-05 ... 4.4703665e-08\n",
            "   1.5344618e-09 1.0090409e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.9683114e-06 3.4726324e-06 1.2326460e-05 ... 1.8200915e-05\n",
            "   5.0476224e-06 7.4788718e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.5036899e-04 2.0278832e-09 1.8697978e-05 ... 4.4830912e-08\n",
            "   1.5480084e-09 1.0147891e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0364857e-06 3.4365323e-06 1.2311343e-05 ... 1.8071911e-05\n",
            "   5.0117164e-06 7.4317422e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4579862e-04 2.0327353e-09 1.8468525e-05 ... 4.4876671e-08\n",
            "   1.5556416e-09 1.0176382e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0688997e-06 3.4105524e-06 1.2300403e-05 ... 1.7972803e-05\n",
            "   4.9834712e-06 7.3941942e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4284420e-04 2.0349451e-09 1.8338827e-05 ... 4.4883659e-08\n",
            "   1.5597464e-09 1.0188922e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.08326105e-06 3.39293433e-06 1.22921165e-05 ... 1.79028229e-05\n",
            "   4.96332450e-06 7.36706943e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.4094552e-04 2.0358337e-09 1.8264142e-05 ... 4.4875861e-08\n",
            "   1.5618838e-09 1.0193507e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0888735e-06 3.3813806e-06 1.2285591e-05 ... 1.7855615e-05\n",
            "   4.9496603e-06 7.3484061e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3972479e-04 2.0360644e-09 1.8220106e-05 ... 4.4863619e-08\n",
            "   1.5629369e-09 1.0194215e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0904629e-06 3.3739641e-06 1.2280409e-05 ... 1.7824659e-05\n",
            "   4.9406694e-06 7.3359115e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3893640e-04 2.0359940e-09 1.8193452e-05 ... 4.4851504e-08\n",
            "   1.5634019e-09 1.0193204e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0903433e-06 3.3692645e-06 1.2276302e-05 ... 1.7804719e-05\n",
            "   4.9348514e-06 7.3276497e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3842286e-04 2.0358020e-09 1.8176803e-05 ... 4.4841116e-08\n",
            "   1.5635675e-09 1.0191581e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0896080e-06 3.3663080e-06 1.2273054e-05 ... 1.7792016e-05\n",
            "   4.9311225e-06 7.3222391e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3808456e-04 2.0355841e-09 1.8166053e-05 ... 4.4832721e-08\n",
            "   1.5635789e-09 1.0189887e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0887448e-06 3.3644551e-06 1.2270487e-05 ... 1.7783967e-05\n",
            "   4.9287419e-06 7.3186757e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3785874e-04 2.0353774e-09 1.8158889e-05 ... 4.4826237e-08\n",
            "   1.5635244e-09 1.0188345e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0879354e-06 3.3632948e-06 1.2268467e-05 ... 1.7778888e-05\n",
            "   4.9272162e-06 7.3163196e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3770551e-04 2.0351982e-09 1.8153951e-05 ... 4.4821363e-08\n",
            "   1.5634466e-09 1.0187003e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0872551e-06 3.3625652e-06 1.2266870e-05 ... 1.7775674e-05\n",
            "   4.9262321e-06 7.3147535e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3759914e-04 2.0350444e-09 1.8150415e-05 ... 4.4817668e-08\n",
            "   1.5633611e-09 1.0185902e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0866994e-06 3.3621047e-06 1.2265612e-05 ... 1.7773666e-05\n",
            "   4.9256000e-06 7.3136962e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.37525213e-04 2.03492934e-09 1.81478845e-05 ... 4.48149287e-08\n",
            "   1.56328783e-09 1.01850155e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0862574e-06 3.3618114e-06 1.2264601e-05 ... 1.7772369e-05\n",
            "   4.9251794e-06 7.3129754e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3747204e-04 2.0348321e-09 1.8145996e-05 ... 4.4812872e-08\n",
            "   1.5632221e-09 1.0184315e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0859109e-06 3.3616261e-06 1.2263818e-05 ... 1.7771557e-05\n",
            "   4.9249029e-06 7.3124802e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3743359e-04 2.0347588e-09 1.8144576e-05 ... 4.4811333e-08\n",
            "   1.5631714e-09 1.0183771e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0856398e-06 3.3615083e-06 1.2263191e-05 ... 1.7771037e-05\n",
            "   4.9247169e-06 7.3121273e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3740539e-04 2.0347011e-09 1.8143512e-05 ... 4.4810196e-08\n",
            "   1.5631243e-09 1.0183348e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0854275e-06 3.3614303e-06 1.2262694e-05 ... 1.7770692e-05\n",
            "   4.9245882e-06 7.3118808e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3738484e-04 2.0346576e-09 1.8142708e-05 ... 4.4809450e-08\n",
            "   1.5630878e-09 1.0183014e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0852574e-06 3.3613799e-06 1.2262290e-05 ... 1.7770493e-05\n",
            "   4.9245009e-06 7.3117017e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3736922e-04 2.0346247e-09 1.8142086e-05 ... 4.4808768e-08\n",
            "   1.5630597e-09 1.0182733e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0851287e-06 3.3613478e-06 1.2261985e-05 ... 1.7770357e-05\n",
            "   4.9244391e-06 7.3115684e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3735749e-04 2.0345985e-09 1.8141611e-05 ... 4.4808363e-08\n",
            "   1.5630366e-09 1.0182544e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0850218e-06 3.3613244e-06 1.2261723e-05 ... 1.7770266e-05\n",
            "   4.9243949e-06 7.3114688e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3734838e-04 2.0345772e-09 1.8141214e-05 ... 4.4808026e-08\n",
            "   1.5630204e-09 1.0182380e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0849368e-06 3.3613119e-06 1.2261526e-05 ... 1.7770217e-05\n",
            "   4.9243677e-06 7.3113993e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.37341655e-04 2.03456119e-09 1.81409305e-05 ... 4.48077522e-08\n",
            "   1.56300484e-09 1.01822595e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0848718e-06 3.3612996e-06 1.2261366e-05 ... 1.7770171e-05\n",
            "   4.9243408e-06 7.3113529e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3733656e-04 2.0345481e-09 1.8140710e-05 ... 4.4807553e-08\n",
            "   1.5629977e-09 1.0182175e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0848213e-06 3.3612971e-06 1.2261238e-05 ... 1.7770157e-05\n",
            "   4.9243226e-06 7.3113124e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3733252e-04 2.0345399e-09 1.8140534e-05 ... 4.4807415e-08\n",
            "   1.5629856e-09 1.0182094e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0847785e-06 3.3612937e-06 1.2261145e-05 ... 1.7770139e-05\n",
            "   4.9243131e-06 7.3112765e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732940e-04 2.0345348e-09 1.8140401e-05 ... 4.4807297e-08\n",
            "   1.5629784e-09 1.0182030e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0847444e-06 3.3612916e-06 1.2261066e-05 ... 1.7770128e-05\n",
            "   4.9243049e-06 7.3112583e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732702e-04 2.0345301e-09 1.8140290e-05 ... 4.4807280e-08\n",
            "   1.5629750e-09 1.0181987e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0847181e-06 3.3612889e-06 1.2260998e-05 ... 1.7770131e-05\n",
            "   4.9243013e-06 7.3112378e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732536e-04 2.0345279e-09 1.8140219e-05 ... 4.4807194e-08\n",
            "   1.5629703e-09 1.0181957e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846967e-06 3.3612873e-06 1.2260945e-05 ... 1.7770120e-05\n",
            "   4.9242935e-06 7.3112210e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732396e-04 2.0345232e-09 1.8140161e-05 ... 4.4807219e-08\n",
            "   1.5629668e-09 1.0181934e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846789e-06 3.3612889e-06 1.2260905e-05 ... 1.7770129e-05\n",
            "   4.9242917e-06 7.3112105e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732277e-04 2.0345201e-09 1.8140097e-05 ... 4.4807152e-08\n",
            "   1.5629644e-09 1.0181918e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.08466485e-06 3.36128687e-06 1.22608735e-05 ... 1.77701186e-05\n",
            "   4.92428899e-06 7.31120599e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732201e-04 2.0345192e-09 1.8140056e-05 ... 4.4807130e-08\n",
            "   1.5629638e-09 1.0181895e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846526e-06 3.3612860e-06 1.2260848e-05 ... 1.7770117e-05\n",
            "   4.9242835e-06 7.3111978e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732105e-04 2.0345166e-09 1.8140034e-05 ... 4.4807070e-08\n",
            "   1.5629616e-09 1.0181881e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846476e-06 3.3612882e-06 1.2260821e-05 ... 1.7770129e-05\n",
            "   4.9242817e-06 7.3111951e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732038e-04 2.0345208e-09 1.8140003e-05 ... 4.4807081e-08\n",
            "   1.5629590e-09 1.0181864e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846394e-06 3.3612880e-06 1.2260807e-05 ... 1.7770124e-05\n",
            "   4.9242858e-06 7.3111951e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3732003e-04 2.0345132e-09 1.8139985e-05 ... 4.4807042e-08\n",
            "   1.5629590e-09 1.0181864e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846362e-06 3.3612887e-06 1.2260799e-05 ... 1.7770128e-05\n",
            "   4.9242826e-06 7.3111892e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731980e-04 2.0345139e-09 1.8139974e-05 ... 4.4807010e-08\n",
            "   1.5629594e-09 1.0181868e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846298e-06 3.3612896e-06 1.2260780e-05 ... 1.7770119e-05\n",
            "   4.9242785e-06 7.3111846e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731965e-04 2.0345139e-09 1.8139956e-05 ... 4.4807010e-08\n",
            "   1.5629594e-09 1.0181848e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846230e-06 3.3612873e-06 1.2260771e-05 ... 1.7770124e-05\n",
            "   4.9242803e-06 7.3111869e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.37319449e-04 2.03451345e-09 1.81399519e-05 ... 4.48070026e-08\n",
            "   1.56295921e-09 1.01818465e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.08462074e-06 3.36128892e-06 1.22607635e-05 ... 1.77701131e-05\n",
            "   4.92428262e-06 7.31118280e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731910e-04 2.0345134e-09 1.8139935e-05 ... 4.4806999e-08\n",
            "   1.5629591e-09 1.0181846e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846212e-06 3.3612857e-06 1.2260764e-05 ... 1.7770129e-05\n",
            "   4.9242776e-06 7.3111833e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.37319158e-04 2.03451189e-09 1.81399373e-05 ... 4.48070132e-08\n",
            "   1.56295799e-09 1.01818385e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846185e-06 3.3612869e-06 1.2260759e-05 ... 1.7770122e-05\n",
            "   4.9242794e-06 7.3111787e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.37319158e-04 2.03451189e-09 1.81399373e-05 ... 4.48070523e-08\n",
            "   1.56295799e-09 1.01818385e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846162e-06 3.3612880e-06 1.2260763e-05 ... 1.7770128e-05\n",
            "   4.9242813e-06 7.3111737e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731910e-04 2.0345134e-09 1.8139935e-05 ... 4.4806999e-08\n",
            "   1.5629562e-09 1.0181827e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846162e-06 3.3612880e-06 1.2260763e-05 ... 1.7770128e-05\n",
            "   4.9242813e-06 7.3111737e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731907e-04 2.0345132e-09 1.8139915e-05 ... 4.4806995e-08\n",
            "   1.5629561e-09 1.0181826e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846162e-06 3.3612880e-06 1.2260750e-05 ... 1.7770128e-05\n",
            "   4.9242763e-06 7.3111737e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731907e-04 2.0345132e-09 1.8139915e-05 ... 4.4806995e-08\n",
            "   1.5629531e-09 1.0181826e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846162e-06 3.3612880e-06 1.2260750e-05 ... 1.7770128e-05\n",
            "   4.9242813e-06 7.3111737e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731907e-04 2.0345132e-09 1.8139915e-05 ... 4.4806995e-08\n",
            "   1.5629531e-09 1.0181826e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846162e-06 3.3612880e-06 1.2260750e-05 ... 1.7770128e-05\n",
            "   4.9242813e-06 7.3111737e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731904e-04 2.0345130e-09 1.8139914e-05 ... 4.4806992e-08\n",
            "   1.5629530e-09 1.0181825e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.0846162e-06 3.3612878e-06 1.2260749e-05 ... 1.7770126e-05\n",
            "   4.9242808e-06 7.3111732e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[3.3731898e-04 2.0345128e-09 1.8139910e-05 ... 4.4806985e-08\n",
            "   1.5629527e-09 1.0181823e-08]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[1.2151930e-05 2.3470078e-07 1.8216044e-05 ... 2.5263146e-06\n",
            "   3.2038852e-07 6.7596238e-07]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[6.8639661e-04 1.0953590e-09 3.7766236e-05 ... 2.6832410e-08\n",
            "   8.5035023e-10 5.7551652e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[2.7591723e-06 1.5132663e-06 9.8450628e-06 ... 1.0192376e-05\n",
            "   1.8781695e-06 3.4410180e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.5815061e-04 1.3764205e-09 2.7433765e-05 ... 3.2646216e-08\n",
            "   9.9444009e-10 6.8537820e-09]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[2.2100612e-06 2.6920807e-06 1.1126571e-05 ... 1.4697387e-05\n",
            "   3.5190233e-06 5.7196735e-06]]]\n",
            "---------\n",
            "(1, 1, 8777) [[[4.5619567e-04 1.7072108e-09 2.7948387e-05 ... 3.8644853e-08\n",
            "   1.2351301e-09 8.5826644e-09]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f82f67adf1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-e6aea88f44a4>\u001b[0m in \u001b[0;36mmean_bleu\u001b[0;34m(x_train, y_train, encoder_model, decoder_model, attention_model, start_i, end_i)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         out, att = decode_sequence(x_train[i][np.newaxis,:], bos_eos, 100,\n\u001b[0;32m---> 43\u001b[0;31m                                    encoder_model, decoder_model, attention_model)\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mout_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0matt_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e6aea88f44a4>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq, bos_eos, max_output_length, encoder_model, decoder_model, attention_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdecoded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstates_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#開始を表す一文字と encoder_model の状態のリスト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZW8D-uvStEX",
        "colab": {}
      },
      "source": [
        "# check prediction\n",
        "def check_test_pred(x_test, encoder_model, decoder_model, attention_model,\n",
        "                    start_i=0, end_i=None):\n",
        "    \"\"\"test データの文書と翻訳結果を見る\"\"\"\n",
        "    out_seqs, att_seqs = [], []\n",
        "    bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
        "    if end_i is None:\n",
        "        end_i = len(x_train)\n",
        "    for i in range(len(x_test[:end_i])):\n",
        "        out, att = decode_sequence(x_test[i][np.newaxis,:], bos_eos, 100,\n",
        "                                   encoder_model, decoder_model, attention_model)\n",
        "        out_seqs.append(out)\n",
        "        att_seqs.append(att)\n",
        "\n",
        "    pred_test = [out_seqs[i][1:-1] for i in range(len(x_test[:end_i]))]\n",
        "    pred_test = [[tokenizer_ja.index_word[idx] for idx in seq if idx != 0] for seq in pred_test]\n",
        "    test_eng = [[tokenizer_en.index_word[idx] for idx in seq if idx != 0] for seq in x_test]\n",
        "\n",
        "    for test, pred in zip(test_eng, pred_test):\n",
        "        print(\"----------------------------\")\n",
        "        print(f\"eng  sentence: {test[1:-1]}\")\n",
        "        print(f\"pred sentence: {pred}\")\n",
        "        print(\"----------------------------\")\n",
        "    return pred_test, test_eng\n",
        "#pred_test, test_eng = check_test_pred(x_test, encoder_model, decoder_model, attention_model, end_i=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OH0bf5am_aLW",
        "colab": {}
      },
      "source": [
        "# csv出力\n",
        "def write_csv(filepath, x_test, bos_eos,\n",
        "              encoder_model, decoder_model, attention_model):\n",
        "    out_seqs, att_seqs = [], []\n",
        "    for i in range(x_test.shape[0]):\n",
        "        out, att = decode_sequence(x_test[i][np.newaxis,:], bos_eos, 100,\n",
        "                                   encoder_model, decoder_model, attention_model)\n",
        "        out_seqs.append(out)\n",
        "        att_seqs.append(att)\n",
        "        #print(out)\n",
        "    output = [out_seqs[i][1:-1] for i in range(len(x_test))]\n",
        "    #output = [decode_sequence(x_test[i][np.newaxis,:], bos_eos, 100)[1:-1] for i in range(len(x_test))]\n",
        "    #print(output)\n",
        "    with open(filepath, 'w') as file:\n",
        "        writer = csv.writer(file, lineterminator='\\n')\n",
        "        writer.writerows(output)\n",
        "\n",
        "#filepath = 'colab_attention_exp.csv'\n",
        "#write_csv(filepath, x_test, bos_eos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-B48Q3Cd_aLY",
        "outputId": "0c33e905-1735-4d29-8fa4-5870317ffd39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "SEED = 20000\n",
        "def run_cv(x_train, y_train, test, params={}):\n",
        "    N = 10\n",
        "    kf = KFold(n_splits=N, random_state=SEED)\n",
        "    fold_splits = kf.split(x_train, y_train)\n",
        "    tr_scores = []\n",
        "    val_scores = []\n",
        "    #results = np.zeros((test.shape[0], N))\n",
        "    train_models, encoder_models, decoder_models, attention_models = [], [], [], []\n",
        "    i = 0\n",
        "    max_mean = 0\n",
        "    best_bleu, best_idx = 0, 0\n",
        "    for tr_idx, val_idx in fold_splits:\n",
        "        print(f'Start fold {i+1}/{N}')\n",
        "#         if i!=2:\n",
        "#             i+=1\n",
        "#             continue\n",
        "        tr_X, val_X = x_train[tr_idx, :], x_train[val_idx, :]\n",
        "        tr_y, val_y = y_train[tr_idx, :], y_train[val_idx, :]\n",
        "        print(tr_X.shape, val_X.shape)\n",
        "        print(tr_y.shape, val_y.shape)\n",
        "\n",
        "        epochs=1000\n",
        "        batch_size=512\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=8),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5),\n",
        "            #ModelCheckpoint(filepath=modelpath, monitor='val_acc', save_best_only=True)\n",
        "        ]\n",
        "        train_model, encoder_model, decoder_model, attention_model = build_attention_models()\n",
        "        train_target = np.hstack((tr_y[:, 1:], np.zeros((len(tr_y),1), dtype=np.int32)))\n",
        "        val_target = np.hstack((val_y[:, 1:], np.zeros((len(val_y),1), dtype=np.int32)))\n",
        "        history = train_model.fit([tr_X, tr_y], np.expand_dims(train_target, -1), \n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=([val_X, val_y], val_target),\n",
        "                            callbacks=callbacks\n",
        "                        )\n",
        "        \n",
        "        # BLEU 確認\n",
        "        print(\"Calcurating Val BLEU...\")\n",
        "        _,_, bleu_scores = mean_bleu(val_X, val_y, encoder_model, decoder_model, attention_model, end_i=4000)\n",
        "        if np.mean(bleu_scores) > best_bleu:\n",
        "            best_idx = i\n",
        "            best_bleu = np.mean(bleu_scores)\n",
        "        i+=1\n",
        "        #return train_model, encoder_model, decoder_model, attention_model\n",
        "        train_models.append(train_model)\n",
        "        encoder_models.append(encoder_model)\n",
        "        decoder_models.append(decoder_model)\n",
        "        attention_models.append(attention_model)\n",
        "        \n",
        "        \n",
        "    #print('mean acc: ', sum(val_scores)/len(val_scores))\n",
        "    print(\"-------------- finish CV --------------\")\n",
        "    print(\"best Mean BLEU: \", best_bleu)\n",
        "    return train_models[best_idx], encoder_models[best_idx], decoder_models[best_idx], attention_models[best_idx]\n",
        "best_train_model, best_encoder_model, best_decoder_model, best_attention_model = run_cv(x_train, y_train, x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start fold 1/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_5/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_5/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_6/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_6/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_5/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_6_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_6_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_6_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 40s 916us/sample - loss: 3.6283 - val_loss: 2.6700\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 2.2808 - val_loss: 2.1220\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.9095 - val_loss: 1.8475\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.6699 - val_loss: 1.6720\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.4565 - val_loss: 1.5049\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.2767 - val_loss: 1.3588\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.1264 - val_loss: 1.2760\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.0023 - val_loss: 1.2201\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.8958 - val_loss: 1.1672\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.8028 - val_loss: 1.1301\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.7199 - val_loss: 1.1103\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.6441 - val_loss: 1.0989\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.5740 - val_loss: 1.1104\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5112 - val_loss: 1.0853\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.4513 - val_loss: 1.1083\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.3981 - val_loss: 1.1172\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.3496 - val_loss: 1.1367\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.3055 - val_loss: 1.1429\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 35s 802us/sample - loss: 0.2683 - val_loss: 1.1508\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1829 - val_loss: 1.1062\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1610 - val_loss: 1.1136\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1495 - val_loss: 1.1207\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.46263552018443865\n",
            "Start fold 2/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_7/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_7/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_8/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_9/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_7/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_8_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_8_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_8_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 41s 936us/sample - loss: 3.5393 - val_loss: 2.4330\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 2.1332 - val_loss: 1.9903\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.7464 - val_loss: 1.7048\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.4791 - val_loss: 1.5325\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.2800 - val_loss: 1.4012\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.1283 - val_loss: 1.2917\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.0014 - val_loss: 1.2249\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.8950 - val_loss: 1.1626\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.8026 - val_loss: 1.1272\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.7203 - val_loss: 1.1077\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.6432 - val_loss: 1.1066\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5750 - val_loss: 1.0894\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.5120 - val_loss: 1.0899\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.4535 - val_loss: 1.0852\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.3990 - val_loss: 1.0965\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.3504 - val_loss: 1.0972\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.3061 - val_loss: 1.1116\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.2664 - val_loss: 1.1254\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 36s 809us/sample - loss: 0.2324 - val_loss: 1.1510\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1568 - val_loss: 1.1003\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1342 - val_loss: 1.1081\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1234 - val_loss: 1.1134\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.4653851755658842\n",
            "Start fold 3/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_9/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_9/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_10/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_12/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_9/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_10_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_10_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_10_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 42s 961us/sample - loss: 3.7073 - val_loss: 2.6399\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 2.2425 - val_loss: 2.0101\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.7909 - val_loss: 1.6913\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.4874 - val_loss: 1.4951\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.2695 - val_loss: 1.3504\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.1124 - val_loss: 1.2535\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.9910 - val_loss: 1.1790\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.8905 - val_loss: 1.1318\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.8065 - val_loss: 1.1137\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.7302 - val_loss: 1.0755\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.6626 - val_loss: 1.0518\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.6001 - val_loss: 1.0398\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5426 - val_loss: 1.0371\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.4902 - val_loss: 1.0347\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.4387 - val_loss: 1.0537\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.3922 - val_loss: 1.0523\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.3490 - val_loss: 1.0492\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.3095 - val_loss: 1.0682\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 36s 816us/sample - loss: 0.2745 - val_loss: 1.0820\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1952 - val_loss: 1.0289\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1706 - val_loss: 1.0317\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1581 - val_loss: 1.0395\n",
            "Epoch 23/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1483 - val_loss: 1.0476\n",
            "Epoch 24/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1394 - val_loss: 1.0557\n",
            "Epoch 25/1000\n",
            "44100/44100 [==============================] - 35s 796us/sample - loss: 0.1316 - val_loss: 1.0615\n",
            "Epoch 26/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1170 - val_loss: 1.0610\n",
            "Epoch 27/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1143 - val_loss: 1.0623\n",
            "Epoch 28/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1124 - val_loss: 1.0641\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.4760907800395721\n",
            "Start fold 4/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_11/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_11/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_12/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_15/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_11/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_12_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_12_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_12_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 44s 988us/sample - loss: 3.7692 - val_loss: 2.7960\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 2.2635 - val_loss: 2.0689\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 1.7728 - val_loss: 1.7200\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.4704 - val_loss: 1.4867\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.2582 - val_loss: 1.3335\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.1045 - val_loss: 1.2434\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.9858 - val_loss: 1.1776\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.8865 - val_loss: 1.1512\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.8016 - val_loss: 1.1133\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.7274 - val_loss: 1.0891\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.6592 - val_loss: 1.0756\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.5972 - val_loss: 1.0613\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5390 - val_loss: 1.0533\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.4837 - val_loss: 1.0602\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.4335 - val_loss: 1.0579\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.3866 - val_loss: 1.0609\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.3451 - val_loss: 1.0787\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 36s 818us/sample - loss: 0.3049 - val_loss: 1.0769\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.2184 - val_loss: 1.0302\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1934 - val_loss: 1.0349\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1796 - val_loss: 1.0415\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1690 - val_loss: 1.0476\n",
            "Epoch 23/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1595 - val_loss: 1.0557\n",
            "Epoch 24/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1505 - val_loss: 1.0623\n",
            "Epoch 25/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1350 - val_loss: 1.0603\n",
            "Epoch 26/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1318 - val_loss: 1.0616\n",
            "Epoch 27/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1299 - val_loss: 1.0632\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.46888179473169944\n",
            "Start fold 5/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_13/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_13/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_14/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_18/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_13/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_14_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_14_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_14_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 45s 1ms/sample - loss: 3.6109 - val_loss: 2.5221\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 2.2030 - val_loss: 2.0438\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.7891 - val_loss: 1.7393\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.5128 - val_loss: 1.5411\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.3015 - val_loss: 1.3944\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.1385 - val_loss: 1.2902\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 1.0072 - val_loss: 1.2265\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.8974 - val_loss: 1.1681\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.8035 - val_loss: 1.1314\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.7196 - val_loss: 1.1004\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.6438 - val_loss: 1.0896\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.5744 - val_loss: 1.0906\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5102 - val_loss: 1.1007\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 796us/sample - loss: 0.4527 - val_loss: 1.0933\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.3988 - val_loss: 1.0903\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 36s 824us/sample - loss: 0.3494 - val_loss: 1.1008\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.2507 - val_loss: 1.0443\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.2230 - val_loss: 1.0490\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 0.2083 - val_loss: 1.0572\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1959 - val_loss: 1.0655\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1853 - val_loss: 1.0711\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1750 - val_loss: 1.0788\n",
            "Epoch 23/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1570 - val_loss: 1.0751\n",
            "Epoch 24/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1531 - val_loss: 1.0765\n",
            "Epoch 25/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1507 - val_loss: 1.0782\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.46630181723576014\n",
            "Start fold 6/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_15/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_15/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_16/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_21/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_15/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_16_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_16_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_16_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 47s 1ms/sample - loss: 3.6540 - val_loss: 2.7011\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 2.2852 - val_loss: 2.1246\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 1.9142 - val_loss: 1.8494\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 1.6628 - val_loss: 1.6583\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 1.4121 - val_loss: 1.4538\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 796us/sample - loss: 1.2063 - val_loss: 1.2929\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 1.0553 - val_loss: 1.2299\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.9379 - val_loss: 1.1627\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.8419 - val_loss: 1.1140\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.7594 - val_loss: 1.0906\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.6841 - val_loss: 1.0625\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.6177 - val_loss: 1.0696\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.5562 - val_loss: 1.0560\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.4982 - val_loss: 1.0495\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.4466 - val_loss: 1.0517\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.3985 - val_loss: 1.0629\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.3531 - val_loss: 1.0655\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 796us/sample - loss: 0.3126 - val_loss: 1.0668\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 37s 840us/sample - loss: 0.2769 - val_loss: 1.0911\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.1951 - val_loss: 1.0486\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.1707 - val_loss: 1.0508\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.1580 - val_loss: 1.0601\n",
            "Epoch 23/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.1478 - val_loss: 1.0666\n",
            "Epoch 24/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.1393 - val_loss: 1.0752\n",
            "Epoch 25/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1312 - val_loss: 1.0846\n",
            "Epoch 26/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 0.1171 - val_loss: 1.0826\n",
            "Epoch 27/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1140 - val_loss: 1.0839\n",
            "Epoch 28/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.1125 - val_loss: 1.0857\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.46311508581353633\n",
            "Start fold 7/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_17/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_17/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_18/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_24/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_17/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_18_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_18_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_18_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 48s 1ms/sample - loss: 3.6143 - val_loss: 2.5521\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 2.2411 - val_loss: 2.1119\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.8635 - val_loss: 1.8481\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 1.6013 - val_loss: 1.5960\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 1.3846 - val_loss: 1.4584\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 795us/sample - loss: 1.2059 - val_loss: 1.3247\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 1.0631 - val_loss: 1.2567\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 794us/sample - loss: 0.9443 - val_loss: 1.2090\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.8463 - val_loss: 1.1411\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.7596 - val_loss: 1.1131\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.6817 - val_loss: 1.1056\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.6116 - val_loss: 1.0880\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5474 - val_loss: 1.0897\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.4885 - val_loss: 1.0928\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.4336 - val_loss: 1.0894\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.3839 - val_loss: 1.1010\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 37s 836us/sample - loss: 0.3395 - val_loss: 1.0912\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.2402 - val_loss: 1.0503\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.2142 - val_loss: 1.0574\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.2002 - val_loss: 1.0645\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1879 - val_loss: 1.0720\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1778 - val_loss: 1.0793\n",
            "Epoch 23/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1679 - val_loss: 1.0853\n",
            "Epoch 24/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1506 - val_loss: 1.0837\n",
            "Epoch 25/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1472 - val_loss: 1.0853\n",
            "Epoch 26/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1446 - val_loss: 1.0870\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.4594115977075943\n",
            "Start fold 8/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_19/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_19/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_20/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_27/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_19/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_20_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_20_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_20_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 47s 1ms/sample - loss: 3.5449 - val_loss: 2.4688\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 2.1365 - val_loss: 1.9494\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.7524 - val_loss: 1.6981\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.4896 - val_loss: 1.5144\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 1.2915 - val_loss: 1.3866\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.1358 - val_loss: 1.2839\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 1.0103 - val_loss: 1.2198\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.9025 - val_loss: 1.1593\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.8084 - val_loss: 1.1247\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.7255 - val_loss: 1.1030\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 0.6492 - val_loss: 1.0764\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.5781 - val_loss: 1.0736\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5142 - val_loss: 1.0790\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.4551 - val_loss: 1.0671\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.4016 - val_loss: 1.0872\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 0.3521 - val_loss: 1.0897\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.3075 - val_loss: 1.1062\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.2667 - val_loss: 1.1194\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 37s 842us/sample - loss: 0.2324 - val_loss: 1.1307\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1569 - val_loss: 1.0869\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1344 - val_loss: 1.0927\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1237 - val_loss: 1.1019\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.4671429774311932\n",
            "Start fold 9/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_21/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_21/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_22/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_30/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_21/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_22_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_22_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_22_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 49s 1ms/sample - loss: 3.6808 - val_loss: 2.6922\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 2.2672 - val_loss: 2.0604\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.8642 - val_loss: 1.8074\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.5914 - val_loss: 1.5778\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.3661 - val_loss: 1.4331\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.1870 - val_loss: 1.2986\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.0461 - val_loss: 1.2593\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 0.9350 - val_loss: 1.1656\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.8404 - val_loss: 1.1598\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.7600 - val_loss: 1.1160\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.6849 - val_loss: 1.0816\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.6183 - val_loss: 1.0900\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5551 - val_loss: 1.0763\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.4979 - val_loss: 1.0717\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 790us/sample - loss: 0.4441 - val_loss: 1.0914\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.3951 - val_loss: 1.0838\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.3493 - val_loss: 1.1016\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.3088 - val_loss: 1.1090\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 37s 849us/sample - loss: 0.2722 - val_loss: 1.1266\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1899 - val_loss: 1.0729\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1658 - val_loss: 1.0780\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1536 - val_loss: 1.0845\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.464938412124921\n",
            "Start fold 10/10\n",
            "(44100, 18) (4900, 18)\n",
            "(44100, 18) (4900, 18)\n",
            "[<tf.Tensor 'lstm_23/while/Exit_3:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_23/while/Exit_4:0' shape=(?, 1024) dtype=float32>]\n",
            "decoded_seq:  Tensor(\"lstm_24/transpose_1:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "score, encoded_seq:  Tensor(\"dense_33/BiasAdd:0\", shape=(?, 18, 1024), dtype=float32) Tensor(\"lstm_23/transpose_2:0\", shape=(?, 18, 1024), dtype=float32)\n",
            "Tensor(\"lstm_24_1/transpose_1:0\", shape=(?, 1, 1024), dtype=float32)\n",
            "[<tf.Tensor 'lstm_24_1/while/Exit_2:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'lstm_24_1/while/Exit_3:0' shape=(?, 1024) dtype=float32>]\n",
            "Train on 44100 samples, validate on 4900 samples\n",
            "Epoch 1/1000\n",
            "44100/44100 [==============================] - 49s 1ms/sample - loss: 3.6910 - val_loss: 2.7440\n",
            "Epoch 2/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 2.3098 - val_loss: 2.0323\n",
            "Epoch 3/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.8148 - val_loss: 1.7152\n",
            "Epoch 4/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.5077 - val_loss: 1.4862\n",
            "Epoch 5/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 1.2808 - val_loss: 1.3673\n",
            "Epoch 6/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 1.1192 - val_loss: 1.2712\n",
            "Epoch 7/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.9944 - val_loss: 1.1972\n",
            "Epoch 8/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.8933 - val_loss: 1.1311\n",
            "Epoch 9/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.8071 - val_loss: 1.0963\n",
            "Epoch 10/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.7323 - val_loss: 1.0684\n",
            "Epoch 11/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.6628 - val_loss: 1.0533\n",
            "Epoch 12/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.6007 - val_loss: 1.0455\n",
            "Epoch 13/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.5426 - val_loss: 1.0586\n",
            "Epoch 14/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.4882 - val_loss: 1.0460\n",
            "Epoch 15/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.4386 - val_loss: 1.0532\n",
            "Epoch 16/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.3921 - val_loss: 1.0596\n",
            "Epoch 17/1000\n",
            "44100/44100 [==============================] - 38s 856us/sample - loss: 0.3480 - val_loss: 1.0541\n",
            "Epoch 18/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.2542 - val_loss: 1.0073\n",
            "Epoch 19/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.2283 - val_loss: 1.0116\n",
            "Epoch 20/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.2142 - val_loss: 1.0152\n",
            "Epoch 21/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.2020 - val_loss: 1.0237\n",
            "Epoch 22/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1912 - val_loss: 1.0302\n",
            "Epoch 23/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1813 - val_loss: 1.0376\n",
            "Epoch 24/1000\n",
            "44100/44100 [==============================] - 35s 792us/sample - loss: 0.1634 - val_loss: 1.0357\n",
            "Epoch 25/1000\n",
            "44100/44100 [==============================] - 35s 791us/sample - loss: 0.1602 - val_loss: 1.0371\n",
            "Epoch 26/1000\n",
            "44100/44100 [==============================] - 35s 793us/sample - loss: 0.1576 - val_loss: 1.0377\n",
            "Calcurating Val BLEU...\n",
            "Mean BLEU:  0.46969185472563507\n",
            "-------------- finish CV --------------\n",
            "best Mean BLEU:  0.4760907800395721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZT3QRSzrT6H",
        "outputId": "2bbc0bc5-7c56-48ef-f464-ddcef2ef439b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "_,_, bleu_scores = mean_bleu(x_train, y_train, best_encoder_model, best_decoder_model, best_attention_model, end_i=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean BLEU:  0.947497172457975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4xPywaAAqQTt",
        "colab": {}
      },
      "source": [
        "best_train_model.save('/content/drive/My Drive/Colab Notebooks/train_model.h5', include_optimizer=False)\n",
        "best_encoder_model.save('/content/drive/My Drive/Colab Notebooks/enc_model.h5', include_optimizer=False)\n",
        "best_decoder_model.save('/content/drive/My Drive/Colab Notebooks/dec_model.h5', include_optimizer=False)\n",
        "best_attention_model.save('/content/drive/My Drive/Colab Notebooks/att_model.h5', include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H7b773vs70yv",
        "outputId": "f4af63bf-f933-4527-9043-d3d95780290e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "_, _ = check_test_pred(x_test, best_encoder_model, best_decoder_model, best_attention_model, end_i=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------\n",
            "eng  sentence: ['he', 'could', 'not', 'believe', 'his', 'ears', '.']\n",
            "pred sentence: ['彼', 'は', '自分', 'の', '言', 'う', 'こと', 'が', '信', 'じ', 'られ', 'な', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['my', 'mother', 'is', 'to', 'meet', 'with', 'my', 'homeroom', 'teacher', 'tomorrow', '.']\n",
            "pred sentence: ['母', 'は', '明日', 'の', '先生', 'と', '会', 'う', 'こと', 'に', 'な', 'っ', 'て', 'い', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'don', \"'t\", 'want', 'to', 'be', 'involved', 'in', 'that', 'matter', '.']\n",
            "pred sentence: ['その', '問題', 'に', 'は', 'これ', 'ら', 'な', 'く', 'て', 'は', 'な', 'ら', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'took', 'me', 'two', 'hours', 'to', 'get', 'to', 'yokohama', '.']\n",
            "pred sentence: ['私', 'は', '東京', 'へ', '行', 'く', 'の', 'に', '２', '時間', 'かか', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'expected', 'that', 'he', 'would', 'come', '.']\n",
            "pred sentence: ['彼', 'が', '来る', 'と', '思', 'っ', 'て', 'い', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['an', 'old', 'man', 'sat', 'next', 'to', 'me', 'on', 'the', 'bus', '.']\n",
            "pred sentence: ['旅行', '者', 'が', '私', 'の', '家', 'に', '座', 'っ', 'て', 'い', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['mr', 'jordan', 'was', 'a', 'little', 'surprised', '.']\n",
            "pred sentence: ['突然', '先生', 'は', '驚', 'い', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'persuaded', 'him', 'to', 'take', 'part', 'in', 'it', '.']\n",
            "pred sentence: ['私', 'は', '彼', 'を', '取り', 'に', 'や', 'る', 'よう', 'に', '説得', 'し', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['we', 'were', 'astonished', 'at', 'the', 'news', '.']\n",
            "pred sentence: ['私', 'たち', 'は', 'その', '知らせ', 'に', '驚', 'い', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['something', 'is', 'the', 'matter', 'with', 'him', 'today', '.']\n",
            "pred sentence: ['今日', 'は', '彼', 'の', '具合', 'が', '悪', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['war', 'is', 'not', 'inevitable', '.']\n",
            "pred sentence: ['戦争', 'は', '決して', '確実', 'で', 'は', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'don', \"'t\", 'think', 'that', 'he', 'will', 'come', '.']\n",
            "pred sentence: ['彼', 'は', '来る', 'と', 'は', '思', 'わ', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'walked', 'to', 'school', '.']\n",
            "pred sentence: ['私', 'は', '学校', 'へ', '歩', 'い', 'て', 'い', 'き', 'ま', 'し', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'make', 'it', 'a', 'rule', 'to', 'do', 'some', 'exercise', 'before', 'breakfast', '.']\n",
            "pred sentence: ['私', 'は', '朝食', '前', 'に', '何', 'か', '運動', 'する', 'こと', 'に', 'し', 'て', 'い', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'ran', 'as', 'fast', 'as', 'possible', 'to', 'catch', 'up', 'with', 'him', '.']\n",
            "pred sentence: ['私', 'は', '彼', 'に', '追いつ', 'く', 'ため', 'に', '速', 'く', '走', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', \"'ll\", 'get', 'you', 'off', 'from', 'work', '.']\n",
            "pred sentence: ['君', 'から', '仕事', 'を', 'さ', 'せ', 'よ', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['how', 'do', 'you', 'want', 'them', '?']\n",
            "pred sentence: ['彼', 'ら', 'は', '何', 'を', '求め', 'て', 'い', 'る', 'の', 'で', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['would', 'you', 'like', 'me', 'to', 'explain', 'it', '?']\n",
            "pred sentence: ['私', 'が', 'それ', 'を', '説明', 'し', 'て', 'い', 'ま', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'poured', 'for', 'three', 'days', '.']\n",
            "pred sentence: ['三', '日', 'で', '着', 'い', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['the', 'cold', 'wind', 'cut', 'through', 'his', 'coat', '.']\n",
            "pred sentence: ['寒', 'い', '風', 'が', '彼', 'の', 'コート', 'を', '切', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'agree', 'to', 'this', 'plan', '.']\n",
            "pred sentence: ['私', 'は', 'この', '計画', 'に', '賛成', 'で', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'can', \"'t\", 'go', 'until', 'he', 'comes', '.']\n",
            "pred sentence: ['彼', 'が', '来る', 'まで', 'は', '、', '私', 'が', 'でき', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['the', 'obverse', 'of', 'peace', 'is', 'war', '.']\n",
            "pred sentence: ['平和', 'の', '成績', 'は', '戦争', 'で', 'あ', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'could', 'do', 'nothing', 'but', 'watch', '.']\n",
            "pred sentence: ['彼', 'は', 'ただ', 'の', '買え', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['be', 'kind', 'to', 'old', 'people', '.']\n",
            "pred sentence: ['お', '年寄り', 'に', 'は', '親切', 'に', 'し', 'なさ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'do', 'not', 'watch', 'television', '.']\n",
            "pred sentence: ['私', 'は', 'テレビ', 'を', '見', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['that', 'bus', 'will', 'take', 'you', 'to', 'the', 'zoo', '.']\n",
            "pred sentence: ['その', 'バス', 'に', '乗', 'る', 'と', 'その', '動物', 'は', '動物', 'で', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'think', 'it', 'needs', 'a', 'tune-up', '.']\n",
            "pred sentence: ['それ', 'は', '必要', 'と', '思', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['how', 'long', 'are', 'you', 'going', 'to', 'stay', 'in', 'japan', '?']\n",
            "pred sentence: ['あなた', 'は', 'どれ', 'くらい', '日本', 'に', '滞在', 'する', 'つもり', 'で', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['come', 'and', 'see', 'me', 'at', 'eleven', 'o', \"'clock\", '.']\n",
            "pred sentence: ['で', 'は', '、', 'うち', 'に', '会', 'い', 'に', '来', 'て', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'haven', \"'t\", 'read', 'both', 'of', 'her', 'novels', '.']\n",
            "pred sentence: ['私', 'は', '彼女', 'の', '小説', 'を', '２', 'つ', '読', 'ん', 'で', 'い', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'was', 'made', 'to', 'stay', 'at', 'home', 'by', 'mother', '.']\n",
            "pred sentence: ['母', 'は', '母', 'に', '家', 'で', '行', 'か', 'れ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'am', 'a', 'student', '.']\n",
            "pred sentence: ['私', 'は', '学生', 'で', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['what', 'have', 'you', 'done', 'with', 'my', 'pen', '?']\n",
            "pred sentence: ['私', 'の', 'ペン', 'は', 'ペン', 'で', 'し', 'た', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['let', \"'s\", 'help', 'each', 'other', '.']\n",
            "pred sentence: ['お', '手伝い', 'を', 'し', 'よ', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['the', 'computer', 'was', 'very', 'useful', '.']\n",
            "pred sentence: ['その', 'コンピューター', 'は', 'とても', '役', 'に', '立', 'た', 'な', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['may', 'i', 'use', 'your', 'toilet', '?']\n",
            "pred sentence: ['トイレ', 'を', 'お', '借り', 'し', 'て', 'い', 'い', '？']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['don', \"'t\", 'speak', 'ill', 'of', 'others', 'behind', 'their', 'back', '.']\n",
            "pred sentence: ['陰', 'で', '人', 'の', '悪口', 'を', '言', 'っ', 'て', 'は', 'いけ', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['please', 'let', 'me', 'know', '.']\n",
            "pred sentence: ['私', 'に', '教え', 'て', 'くださ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['no', 'matter', 'what', 'happens', ',', 'i', \"'ll\", 'stand', 'by', 'you', '.']\n",
            "pred sentence: ['何', 'が', '起こ', 'っ', 'て', 'も', '、', '私', 'は', 'あなた', 'を', '助け', 'て', 'お', 'き', 'ま', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'was', 'embarrassed', '.']\n",
            "pred sentence: ['彼', 'は', '困惑', 'し', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['how', 'many', 'times', 'a', 'year', 'do', 'you', 'go', 'skiing', '?']\n",
            "pred sentence: ['あなた', 'は', '１', '年', 'に', '何', '回', '何', '本', '食べ', 'ま', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'became', 'famous', '.']\n",
            "pred sentence: ['彼', 'は', '有名', 'に', 'な', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['she', 'is', 'now', 'an', 'utter', 'stranger', 'to', 'me', '.']\n",
            "pred sentence: ['彼女', 'は', '全く', '私', 'の', '知', 'ら', 'な', 'い', '人', 'で', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'took', 'as', 'much', 'care', 'as', 'possible', '.']\n",
            "pred sentence: ['彼', 'は', 'でき', 'る', 'だけ', 'たくさん', 'の', '世話', 'を', 'し', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['as', 'soon', 'as', 'he', 'took', 'the', 'medicine', ',', 'his', 'fever', 'went', 'down', '.']\n",
            "pred sentence: ['彼', 'は', '薬', 'を', '飲', 'む', 'と', 'すぐ', 'に', '眠', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['she', 'did', 'not', 'take', 'kindly', 'to', 'my', 'advice', '.']\n",
            "pred sentence: ['彼女', 'は', '私', 'の', '忠告', 'に', '従', 'わ', 'な', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['then', 'she', 'began', 'to', 'walk', 'again', '.']\n",
            "pred sentence: ['彼女', 'は', 'また', '歩', 'き', '始め', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['can', 'i', 'make', 'a', 'reservation', '?']\n",
            "pred sentence: ['予約', 'し', 'て', 'もらえ', 'ま', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['what', 'do', 'you', 'say', 'to', 'going', 'for', 'a', 'drive', '?']\n",
            "pred sentence: ['ドライブ', 'に', '行', 'く', 'の', 'は', 'どう', 'で', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', \"'d\", 'like', 'a', 'table', 'by', 'the', 'window', '.']\n",
            "pred sentence: ['窓側', 'の', '席', 'が', 'い', 'い', 'の', 'で', 'す', 'が', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['his', 'joke', 'has', 'been', 'done', 'to', 'death', '.']\n",
            "pred sentence: ['彼', 'の', '冗談', 'は', '死', 'ん', 'だ', 'の', 'で', 'い', 'ま', 'せ', 'ん', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['his', 'car', 'was', 'blue', ';', 'hers', 'was', 'red', '.']\n",
            "pred sentence: ['彼', 'の', '車', 'は', '赤', 'い', 'て', '赤', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'is', 'famous', 'as', 'a', 'doctor', '.']\n",
            "pred sentence: ['彼', 'は', '医者', 'と', 'し', 'て', '有名', 'で', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['go', 'ahead', '.', 'your', 'party', 'is', 'on', 'the', 'line', '.']\n",
            "pred sentence: ['いらっしゃ', 'い', '、', '出', 'な', 'さ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', \"'ll\", 'see', 'you', 'later', '.']\n",
            "pred sentence: ['また', '後', 'で', '会', 'い', 'ま', 'しょ', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'was', 'not', 'long', 'before', 'she', 'came', 'back', '.']\n",
            "pred sentence: ['間', 'も', 'な', 'く', '彼女', 'は', '帰', 'っ', 'て', 'き', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['you', 'deserve', 'to', 'succeed', '.']\n",
            "pred sentence: ['君', 'は', '成功', 'し', 'な', 'けれ', 'ば', 'な', 'ら', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'will', 'be', 'winter', 'before', 'long', '.']\n",
            "pred sentence: ['ま', 'も', 'な', 'く', '冬', 'は', 'かか', 'る', 'だ', 'ろ', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'makes', 'no', 'difference', 'whether', 'you', 'go', 'today', 'or', 'tomorrow', '.']\n",
            "pred sentence: ['君', 'が', '今日', '行', 'く', 'か', 'どう', 'か', 'は', 'どう', 'で', 'も', 'い', 'い', 'こと', 'だ', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'is', 'apt', 'to', 'catch', 'cold', '.']\n",
            "pred sentence: ['彼', 'は', '寒', 'さ', 'に', '慣れ', 'て', 'い', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'was', 'not', 'until', 'a', 'few', 'days', 'later', 'that', 'he', 'arrived', '.']\n",
            "pred sentence: ['彼', 'が', '到着', 'し', 'た', 'まで', 'あと', 'は', 'ほとんど', '雨', 'が', '少な', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['we', 'alternated', 'with', 'each', 'other', 'in', 'driving', 'the', 'car', '.']\n",
            "pred sentence: ['我々', 'は', '車', 'で', '足', 'の', 'そば', 'を', '泊ま', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'have', 'to', 'go', 'to', 'the', 'bank', '.']\n",
            "pred sentence: ['私', 'は', '銀行', 'へ', '行', 'か', 'な', 'けれ', 'ば', 'な', 'ら', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['is', 'he', 'anything', 'like', 'handsome', '?']\n",
            "pred sentence: ['彼', 'は', 'どんな', '種類', 'の', 'もの', 'で', 'す', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'didn', \"'t\", 'go', ',', 'but', 'stayed', 'at', 'home', '.']\n",
            "pred sentence: ['私', 'は', '行', 'か', 'な', 'かっ', 'た', 'の', 'で', '家', 'に', 'い', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['he', 'wanted', 'to', 'come', 'with', 'us', '.']\n",
            "pred sentence: ['彼', 'は', '私', 'たち', 'と', '一緒', 'に', 'き', 'た', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['are', 'you', 'through', 'with', 'the', 'paper', '?']\n",
            "pred sentence: ['新聞', 'は', 'お', 'すみ', 'に', 'な', 'り', 'ま', 'し', 'た', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['weather', 'permitting', 'we', 'will', 'go', 'for', 'a', 'drive', '.']\n",
            "pred sentence: ['天気', 'が', 'よ', 'けれ', 'ば', '私', '達', 'は', '旅行', 'に', '行', 'く', 'つもり', 'だ', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['please', 'don', \"'t\", 'make', 'so', 'much', 'noise', '.']\n",
            "pred sentence: ['そんな', 'に', '騒音', 'に', 'な', 'ら', 'な', 'い', 'で', 'くださ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['you', 'had', 'better', 'look', 'up', 'the', 'word', '.']\n",
            "pred sentence: ['君', 'は', 'その', '単語', 'を', 'よく', '見', 'た', '方', 'が', 'よ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['we', 'were', 'caught', 'in', 'a', 'shower', 'on', 'our', 'way', 'from', 'school', '.']\n",
            "pred sentence: ['私', 'は', '学校', 'から', '帰', 'る', '途中', 'で', 'にわか', '雨', 'に', 'あ', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['she', 'is', 'a', 'poor', 'cook', '.']\n",
            "pred sentence: ['彼女', 'は', '料理', 'が', '上手', 'で', 'す', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['take', 'your', 'hands', 'off', 'me', '.']\n",
            "pred sentence: ['手', 'を', 'ふれ', 'な', 'さ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['what', 'vile', 'behavior', '!']\n",
            "pred sentence: ['何', 'と', 'い', 'う', '意味', 'で', 'しょ', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['this', 'old', 'book', 'is', 'worth', '50,000', 'yen', '.']\n",
            "pred sentence: ['この', '古', 'い', '本', 'は', '十分', 'に', '十分', 'に', 'あ', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'don', \"'t\", 'know', 'what', 'to', 'do', '.']\n",
            "pred sentence: ['私', 'は', '何', 'を', 'し', 'たら', 'よ', 'い', 'か', 'わか', 'ら', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['you', 'can', 'get', 'over', 'the', 'handicap', 'soon', '.']\n",
            "pred sentence: ['もう', 'すぐ', '終わ', 'る', 'ぞ', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['you', 'should', 'only', 'buy', 'such', 'things', 'as', 'you', 'need', 'for', 'your', 'everyday', 'life', '.']\n",
            "pred sentence: ['自分', 'で', 'も', '自分', 'で', 'そんな', '事', 'を', '買', 'う', 'べ', 'き', 'だ', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['mother', 'looked', 'at', 'me', 'with', 'tears', 'in', 'her', 'eyes', '.']\n",
            "pred sentence: ['母', 'は', '目', 'に', '涙', 'を', '浮かべ', 'て', '私', 'を', '見', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['the', 'boy', 'got', 'in', 'through', 'the', 'window', '.']\n",
            "pred sentence: ['その', '少年', 'は', '窓', 'から', '後', 'に', '入', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['you', 'should', 'have', 'seen', 'the', 'film', '.']\n",
            "pred sentence: ['あの', '映画', 'を', '見', 'る', 'べ', 'き', 'だっ', 'た', 'の', 'に', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['the', 'man', 'asked', 'me', 'for', 'some', 'money', '.']\n",
            "pred sentence: ['その', '男', 'は', '私', 'に', '金', 'を', 'くれ', 'と', '頼', 'ん', 'だ', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['drop', 'me', 'a', 'line', '.']\n",
            "pred sentence: ['ちょっと', '書', 'い', 'て', 'くださ', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'have', 'two', 'foreign', 'friends', '.']\n",
            "pred sentence: ['友人', 'は', '二', '度', '友人', 'で', 'あ', 'る', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['it', 'was', 'very', 'kind', 'of', 'you', 'to', 'visit', 'me', 'when', 'i', 'was', 'ill', '.']\n",
            "pred sentence: ['私', 'が', '病気', 'に', 'な', 'っ', 'て', 'くださ', 'っ', 'て', '本当', 'に', 'ありがとう', 'ござ', 'い', 'ま', 'し', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['she', 'went', 'to', 'the', 'cinema', 'the', 'other', 'day', '.']\n",
            "pred sentence: ['彼女', 'は', 'その', '日', 'に', '映画', 'を', '見', 'に', '行', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['they', \"'ll\", 'walk', 'there', 'in', 'half', 'an', 'hour', '.']\n",
            "pred sentence: ['彼', 'ら', 'は', '１', '時間', 'で', 'そこ', 'へ', '行', 'く', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['they', 'discuss', 'the', 'matter', 'every', 'day', '.']\n",
            "pred sentence: ['彼', 'ら', 'は', '毎日', 'その', '問題', 'を', '議論', 'し', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['as', 'you', 'are', 'sorry', ',', 'i', \"'ll\", 'forgive', 'you', '.']\n",
            "pred sentence: ['君', 'が', '悪', 'い', 'の', 'で', 'ご', '連絡', 'かけ', 'ま', 'しょ', 'う', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['don', \"'t\", 'look', 'into', 'my', 'room', '.']\n",
            "pred sentence: ['私', 'の', '部屋', 'を', '見', 'て', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'prefer', 'coffee', 'to', 'tea', '.']\n",
            "pred sentence: ['私', 'は', '紅茶', 'より', 'コーヒー', 'が', '好き', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['her', 'tastes', 'in', 'clothes', 'are', 'quite', 'different', 'than', 'mine', '.']\n",
            "pred sentence: ['彼女', 'は', '服', 'と', '同じ', 'くらい', '高', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['the', 'boy', 'could', 'not', 'live', 'up', 'to', 'the', 'school', 'rules', '.']\n",
            "pred sentence: ['その', '少年', 'は', '学校', 'の', '生徒', 'に', '乗', 'る', 'しか', '生き', 'られ', 'な', 'かっ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['can', 'we', 'make', 'a', 'lunch', 'date', '?']\n",
            "pred sentence: ['昼食', 'を', 'と', 'る', 'こと', 'は', 'でき', 'ま', 'せ', 'ん', 'か', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['she', 'waited', 'for', 'you', 'for', 'two', 'hours', '.']\n",
            "pred sentence: ['彼女', 'は', '２', '時間', '以上', '待', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['i', 'met', 'her', 'at', 'the', 'station', 'by', 'accident', '.']\n",
            "pred sentence: ['私', 'は', '偶然', 'に', '彼女', 'に', '駅', 'で', '会', 'っ', 'た', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['tonight', 'i', \"'ve\", 'got', 'to', 'do', 'get', 'ready', 'for', 'tomorrow', '.']\n",
            "pred sentence: ['今夜', 'は', '、', '明日', 'の', '準備', 'で', 'す', 'よ', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['you', 'must', 'study', 'english', 'every', 'day', '.']\n",
            "pred sentence: ['毎日', '英語', 'を', '勉強', 'し', 'な', 'けれ', 'ば', 'な', 'ら', 'な', 'い', '。']\n",
            "----------------------------\n",
            "----------------------------\n",
            "eng  sentence: ['not', 'less', 'than', 'one', 'hundred', 'people', 'were', 'present', '.']\n",
            "pred sentence: ['参加', 'する', '人', 'は', '少な', 'く', 'とも', '１００', '人', 'た', '。']\n",
            "----------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAeB8_lF8Vue",
        "colab": {}
      },
      "source": [
        "filepath = '/content/drive/My Drive/Colab Notebooks/cv10_attention.csv'\n",
        "write_csv(filepath, x_test, bos_eos, best_encoder_model, best_decoder_model, best_attention_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfvVDTFPqeUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_train_model.save('submission3_train_model.h5', include_optimizer=False)\n",
        "best_encoder_model.save('submission3_enc.h5', include_optimizer=False)\n",
        "best_decoder_model.save('/content/drive/My Drive/Colab Notebooks/dec_model.h5', include_optimizer=False)\n",
        "best_attention_model.save('/content/drive/My Drive/Colab Notebooks/att_model.h5', include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYkbiYep4tnK",
        "colab_type": "code",
        "outputId": "d9e4fcc7-daf1-4b47-e0e3-3f1b64414783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "\tsequences = [[list(), 1.0]] #[[[], 1.0]]\n",
        "\t# walk over each step in sequence\n",
        "\tfor row in data:\n",
        "\t\tall_candidates = list()\n",
        "\t\t# expand each current candidate\n",
        "\t\tfor i in range(len(sequences)):\n",
        "\t\t\tseq, score = sequences[i] #[], 1.0\n",
        "\t\t\tfor j in range(len(row)):\n",
        "                # row = data[0,:]\n",
        "\t\t\t\tcandidate = [seq + [j], score * -log(row[j])] # candidate=[[0], 1.0*-log(0.1) #ありえないものほどスコアがでかくなる\n",
        "                #print('candidate', candidate)\n",
        "\t\t\t\tall_candidates.append(candidate) # all_candidate = [[[0], 1.0*-log(0.1)], [[1], 1.0-log(0.2)], .. ]\n",
        "                #print('all_cand', all_candidates)\n",
        "\t\t# order all candidates by score\n",
        "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\t\t# select k best\n",
        "\t\tsequences = ordered[:k] #[ row_index(word_index), score]\n",
        "\treturn sequences\n",
        "\n",
        " # define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.1, 0.2, 0.3, 0.4, 0.5],]\n",
        "\t\t# [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t# [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t# [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t# [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t# [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t# [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t# [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "\t\t#[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "\t\t#[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data2 = []\n",
        "data = array(data)\n",
        "# decode sequence\n",
        "result = beam_search_decoder(data, 3)\n",
        "# print result\n",
        "for seq in result:\n",
        "\tprint(seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4], 0.6931471805599453]\n",
            "[[3], 0.916290731874155]\n",
            "[[2], 1.2039728043259361]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wfi83OXBg_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}