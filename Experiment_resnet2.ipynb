{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Experiment_resnet2.ipynb ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekity1002/DL4US_homeworks/blob/master/Experiment_resnet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0L_wseaLxy",
        "colab_type": "code",
        "outputId": "2dbc9ed7-a7c5-4f21-fd36-995ab8cc71be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Q-E3vxaCQt",
        "colab_type": "code",
        "outputId": "309d3d78-62ac-4b3b-fc0c-2fed9179ce00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Input, Activation, add, Add, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "SEED = 2019"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHFwBDQpaCQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_cifar10():\n",
        "    \n",
        "    # 学習データ\n",
        "    # x_train = np.load('/root/userspace/public/lesson2/data/x_train.npy')\n",
        "    # y_train = np.load('/root/userspace/public/lesson2/data/y_train.npy')\n",
        "    x_train = np.load('/content/drive/My Drive/Colab Notebooks/x_train.npy')\n",
        "    y_train = np.load('/content/drive/My Drive/Colab Notebooks/y_train.npy')\n",
        "    # テストデータ\n",
        "    # x_test = np.load('/root/userspace/public/lesson2/data/x_test.npy')\n",
        "    x_test = np.load('/content/drive/My Drive/Colab Notebooks/x_test.npy')\n",
        "    \n",
        "    x_train = x_train / 255.\n",
        "    x_test = x_test / 255.\n",
        "\n",
        "    # 平均値を引く\n",
        "    subtract_pixel_mean = True\n",
        "    if subtract_pixel_mean:\n",
        "        x_train_mean = np.mean(x_train, axis=0)\n",
        "        x_train -= x_train_mean\n",
        "        x_test -= x_train_mean\n",
        "        \n",
        "    y_train = np.eye(10)[y_train]    \n",
        "    return (x_train, x_test, y_train)\n",
        "\n",
        "x_train, x_test, y_train = load_cifar10()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ngpQHkaCQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 水増し用ジェネレータ定義\n",
        "def my_generator(x, y, batch_size):\n",
        "    gen_params = {\n",
        "        'rotation_range':20,\n",
        "        'horizontal_flip':True,\n",
        "        'height_shift_range':0.2,\n",
        "        'width_shift_range':0.2,\n",
        "        'zoom_range':0.2,\n",
        "        'channel_shift_range':0.2\n",
        "    }\n",
        "    img_gen = ImageDataGenerator(**gen_params)\n",
        "    for x_batch, y_batch in img_gen.flow(x,y,batch_size=batch_size):\n",
        "        yield x_batch, y_batch\n",
        "        \n",
        "def tta(model, test_size, generator, batch_size ,epochs=10):\n",
        "    # test_time_augmentation 予測画像を水増しし、多数決を取る。\n",
        "    # batch_sizeは，test_sizeの約数でないといけない．\n",
        "    pred = np.zeros(shape = (test_size,10), dtype=float)\n",
        "    step_per_epoch = test_size // batch_size #1バッジに使うテストデータの量\n",
        "    for epoch in range(epochs):\n",
        "        for step in range(step_per_epoch):\n",
        "            sta = batch_size * step #バッジの先頭のデータのテストデータにおけるインデックス ?\n",
        "            end = sta + batch_size  # バッジ終了のテストデータのインデックス\n",
        "            tmp_x = generator.__next__() #予測画像をbatch_size だけ生成\n",
        "            #print(\"tmpx : \", tmp_x.shape)\n",
        "            pred[sta:end] += model.predict(tmp_x)\n",
        "\n",
        "    return pred / epochs\n",
        "\n",
        "\n",
        "def tta_generator(x_test, batch_size):\n",
        "    \"\"\"tta用 予測画像水増し用ジェネレータ\"\"\"\n",
        "    return ImageDataGenerator(rotation_range = 20 , horizontal_flip = True,height_shift_range = 0.2,\\\n",
        "                                 width_shift_range = 0.2,zoom_range = 0.2,channel_shift_range = 0.2\\\n",
        "                                  ).flow(x_test, batch_size = batch_size,shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XECwjbqaCQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_acc_and_loss(history):\n",
        "    \"\"\"plot history\"\"\"\n",
        "    fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
        "    \n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']    \n",
        "    epochs=range(1, len(acc)+1)    \n",
        "    ax[0].plot(epochs, acc, label='Train')\n",
        "    ax[0].plot(epochs, val_acc, label='Val')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_title('Accuracy')\n",
        "    \n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs=range(1, len(loss)+1)\n",
        "    ax[1].plot(epochs, loss, label='Train')\n",
        "    ax[1].plot(epochs, val_loss, label='Val')\n",
        "    ax[1].set_xlim(0, 5)\n",
        "    ax[1].legend()\n",
        "    ax[1].set_title('Loss')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4cgohMKa3xB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shortcut(x, residual):\n",
        "    \n",
        "    n_filters = int(residual.shape[-1]) #チャネル数\n",
        "    \n",
        "    # inputs と residual のチャネル数が違ったときのため 1x1 Conv で\n",
        "    # residual 側のフィルタ数にする\n",
        "    shortcut = Conv2D(n_filters, (1,1), strides=(1,1), padding='valid')(x)\n",
        "    \n",
        "    return add([shortcut, residual])\n",
        "\n",
        "def resblock(filters1, filters2, strides=(1,1)):\n",
        "    \"\"\"\n",
        "    filters1 : ボトルネック チャネル数\n",
        "    filters2 : 出力チャネル数\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        \"\"\"Bottleneck アーキテクチャ\"\"\"\n",
        "        x = Conv2D(filters1, (1,1), padding='same', strides=strides, kernel_initializer='he_normal')(input)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(filters1, (3,3), padding='same', strides=strides, kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(filters2, (1,1), padding='same', strides=strides, kernel_initializer='he_normal')(x)\n",
        "        return _shortcut(input, x)\n",
        "    return f\n",
        "\n",
        "def build_model():\n",
        "    drop_rate=0.3\n",
        "    inp = Input(shape = (32,32,3))\n",
        "    x = Conv2D(64, (7,7), strides=(1,1),\n",
        "               kernel_initializer='he_normal', padding='same')(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2,2), padding='same')(x)\n",
        "\n",
        "    x = resblock(64, 256)(x)\n",
        "    x = resblock(64, 256)(x)\n",
        "    x = resblock(64, 256)(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "\n",
        "    x = resblock(128, 512)(x)\n",
        "    x = resblock(128, 512)(x)\n",
        "    x = resblock(128, 512)(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "\n",
        "    x = resblock(256, 1024)(x)\n",
        "    x = resblock(256, 1024)(x)\n",
        "    x = resblock(256, 1024)(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "\n",
        "    x = resblock(512, 2048)(x)\n",
        "    x = resblock(512, 2048)(x)\n",
        "    x = resblock(512, 2048)(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "\n",
        "    x = Dense(2048,activation = \"relu\",  kernel_initializer='he_normal')(x)\n",
        "    x = Dropout(drop_rate)(x)\n",
        "    x = Dense(516,activation = \"relu\",  kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(1024,activation = \"relu\",  kernel_initializer='he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    out  = Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "    return Model(inputs=inp, outputs=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzO9gLCjaCQ3",
        "colab_type": "code",
        "outputId": "be38a924-fbe3-4ebe-a570-f2d55770a6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "X_trn, X_val, y_trn, y_val = train_test_split(x_train, y_train, random_state=SEED, stratify=y_train)\n",
        "batch_size = 2000\n",
        "epochs = 1000\n",
        "steps_per_epoch = X_trn.shape[0] // batch_size\n",
        "validation_steps = X_val.shape[0] // batch_size\n",
        "callbacks = [EarlyStopping(patience=30),\n",
        "             ReduceLROnPlateau(patience=15, factor=0.2),\n",
        "             #LearningRateScheduler(lr_schedule),\n",
        "             #ModelCheckpoint(filepath='/root/userspace/lesson2/model_1.hdf5', monitor='val_acc', save_best_only=True)\n",
        "            ]\n",
        "\n",
        "#train_gen = ImageDataGenerator().flow(X_trn, y_trn, batch_size)\n",
        "val_gen = ImageDataGenerator().flow(X_val, y_val, batch_size)\n",
        "\n",
        "history = model.fit_generator(my_generator(X_trn, y_trn, batch_size),\n",
        "                    steps_per_epoch=len(X_trn)//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=my_generator(X_val, y_val, batch_size),\n",
        "                    validation_steps=len(X_val)//batch_size,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=2)\n",
        "#model.fit(x=x_train, y=y_train, batch_size=700, epochs=150, validation_split=0.1)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, 1)\n",
        "submission = pd.Series(y_pred, name='label')\n",
        "submission.to_csv('submission_expresnet2.csv', header=True, index_label='id')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Epoch 1/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 124s - loss: 2.3788 - acc: 0.1895 - val_loss: 79.6363 - val_acc: 0.1217\n",
            "Epoch 2/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 108s - loss: 1.9712 - acc: 0.2403 - val_loss: 4.0049 - val_acc: 0.1697\n",
            "Epoch 3/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.8439 - acc: 0.2756 - val_loss: 4.3618 - val_acc: 0.1575\n",
            "Epoch 4/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.7749 - acc: 0.3024 - val_loss: 3.5753 - val_acc: 0.1564\n",
            "Epoch 5/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.7110 - acc: 0.3341 - val_loss: 3.4835 - val_acc: 0.1431\n",
            "Epoch 6/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.6741 - acc: 0.3477 - val_loss: 3.2443 - val_acc: 0.1186\n",
            "Epoch 7/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.6267 - acc: 0.3775 - val_loss: 2.6973 - val_acc: 0.1390\n",
            "Epoch 8/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.5616 - acc: 0.4079 - val_loss: 3.3612 - val_acc: 0.1049\n",
            "Epoch 9/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.4941 - acc: 0.4299 - val_loss: 3.2480 - val_acc: 0.1130\n",
            "Epoch 10/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.4533 - acc: 0.4531 - val_loss: 3.4853 - val_acc: 0.1135\n",
            "Epoch 11/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.3899 - acc: 0.4831 - val_loss: 4.0584 - val_acc: 0.0997\n",
            "Epoch 12/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.3343 - acc: 0.5084 - val_loss: 3.5023 - val_acc: 0.1208\n",
            "Epoch 13/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.2827 - acc: 0.5273 - val_loss: 3.3220 - val_acc: 0.1264\n",
            "Epoch 14/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.2400 - acc: 0.5511 - val_loss: 2.4427 - val_acc: 0.2614\n",
            "Epoch 15/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.2091 - acc: 0.5619 - val_loss: 3.6765 - val_acc: 0.1634\n",
            "Epoch 16/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.1754 - acc: 0.5741 - val_loss: 2.6110 - val_acc: 0.2031\n",
            "Epoch 17/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.1315 - acc: 0.5916 - val_loss: 2.3818 - val_acc: 0.2579\n",
            "Epoch 18/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.0905 - acc: 0.6084 - val_loss: 2.2740 - val_acc: 0.3009\n",
            "Epoch 19/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 1.0826 - acc: 0.6103 - val_loss: 1.7900 - val_acc: 0.4146\n",
            "Epoch 20/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 1.0268 - acc: 0.6336 - val_loss: 1.6908 - val_acc: 0.4587\n",
            "Epoch 21/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.9925 - acc: 0.6465 - val_loss: 1.7642 - val_acc: 0.4305\n",
            "Epoch 22/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.9706 - acc: 0.6545 - val_loss: 1.5015 - val_acc: 0.4975\n",
            "Epoch 23/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.9581 - acc: 0.6593 - val_loss: 1.5550 - val_acc: 0.5181\n",
            "Epoch 24/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.9196 - acc: 0.6746 - val_loss: 1.2026 - val_acc: 0.5855\n",
            "Epoch 25/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.9074 - acc: 0.6776 - val_loss: 1.3202 - val_acc: 0.5650\n",
            "Epoch 26/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.8722 - acc: 0.6909 - val_loss: 1.3617 - val_acc: 0.5712\n",
            "Epoch 27/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.8646 - acc: 0.6980 - val_loss: 1.2186 - val_acc: 0.5953\n",
            "Epoch 28/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.8428 - acc: 0.7055 - val_loss: 1.3034 - val_acc: 0.5768\n",
            "Epoch 29/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.8317 - acc: 0.7090 - val_loss: 1.0868 - val_acc: 0.6381\n",
            "Epoch 30/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.8106 - acc: 0.7169 - val_loss: 1.5192 - val_acc: 0.5632\n",
            "Epoch 31/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.8050 - acc: 0.7228 - val_loss: 1.0556 - val_acc: 0.6622\n",
            "Epoch 32/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7770 - acc: 0.7321 - val_loss: 1.0975 - val_acc: 0.6444\n",
            "Epoch 33/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7798 - acc: 0.7279 - val_loss: 1.1739 - val_acc: 0.6184\n",
            "Epoch 34/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7566 - acc: 0.7377 - val_loss: 0.9868 - val_acc: 0.6728\n",
            "Epoch 35/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7305 - acc: 0.7472 - val_loss: 0.9854 - val_acc: 0.6786\n",
            "Epoch 36/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7125 - acc: 0.7538 - val_loss: 0.9357 - val_acc: 0.7017\n",
            "Epoch 37/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7158 - acc: 0.7520 - val_loss: 1.0664 - val_acc: 0.6650\n",
            "Epoch 38/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.7014 - acc: 0.7588 - val_loss: 1.1375 - val_acc: 0.6655\n",
            "Epoch 39/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.6909 - acc: 0.7626 - val_loss: 1.0947 - val_acc: 0.6630\n",
            "Epoch 40/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6836 - acc: 0.7653 - val_loss: 1.0826 - val_acc: 0.6678\n",
            "Epoch 41/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6705 - acc: 0.7703 - val_loss: 0.9413 - val_acc: 0.7050\n",
            "Epoch 42/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6677 - acc: 0.7695 - val_loss: 1.0672 - val_acc: 0.6804\n",
            "Epoch 43/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6495 - acc: 0.7766 - val_loss: 0.9456 - val_acc: 0.7042\n",
            "Epoch 44/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6447 - acc: 0.7773 - val_loss: 0.8795 - val_acc: 0.7244\n",
            "Epoch 45/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6298 - acc: 0.7871 - val_loss: 0.8611 - val_acc: 0.7298\n",
            "Epoch 46/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6119 - acc: 0.7913 - val_loss: 1.0377 - val_acc: 0.6883\n",
            "Epoch 47/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6013 - acc: 0.7966 - val_loss: 0.9027 - val_acc: 0.7169\n",
            "Epoch 48/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.5895 - acc: 0.7963 - val_loss: 0.8674 - val_acc: 0.7254\n",
            "Epoch 49/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.6028 - acc: 0.7946 - val_loss: 0.7955 - val_acc: 0.7377\n",
            "Epoch 50/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5835 - acc: 0.7972 - val_loss: 0.8534 - val_acc: 0.7307\n",
            "Epoch 51/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5757 - acc: 0.8032 - val_loss: 0.8320 - val_acc: 0.7319\n",
            "Epoch 52/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5749 - acc: 0.8037 - val_loss: 0.7999 - val_acc: 0.7455\n",
            "Epoch 53/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5660 - acc: 0.8073 - val_loss: 0.8510 - val_acc: 0.7398\n",
            "Epoch 54/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5467 - acc: 0.8110 - val_loss: 0.8787 - val_acc: 0.7353\n",
            "Epoch 55/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5398 - acc: 0.8165 - val_loss: 0.8295 - val_acc: 0.7424\n",
            "Epoch 56/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5379 - acc: 0.8165 - val_loss: 0.9492 - val_acc: 0.7344\n",
            "Epoch 57/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5469 - acc: 0.8152 - val_loss: 0.8775 - val_acc: 0.7412\n",
            "Epoch 58/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.5423 - acc: 0.8129 - val_loss: 0.8139 - val_acc: 0.7480\n",
            "Epoch 59/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5265 - acc: 0.8209 - val_loss: 0.8378 - val_acc: 0.7468\n",
            "Epoch 60/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5175 - acc: 0.8254 - val_loss: 0.8303 - val_acc: 0.7487\n",
            "Epoch 61/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5020 - acc: 0.8273 - val_loss: 0.8598 - val_acc: 0.7393\n",
            "Epoch 62/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.4999 - acc: 0.8289 - val_loss: 0.8539 - val_acc: 0.7390\n",
            "Epoch 63/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5083 - acc: 0.8288 - val_loss: 0.9235 - val_acc: 0.7410\n",
            "Epoch 64/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.5063 - acc: 0.8290 - val_loss: 0.8365 - val_acc: 0.7431\n",
            "Epoch 65/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.4482 - acc: 0.8472 - val_loss: 0.6318 - val_acc: 0.8051\n",
            "Epoch 66/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.4018 - acc: 0.8635 - val_loss: 0.5903 - val_acc: 0.8090\n",
            "Epoch 67/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3877 - acc: 0.8686 - val_loss: 0.6157 - val_acc: 0.8031\n",
            "Epoch 68/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3792 - acc: 0.8694 - val_loss: 0.6307 - val_acc: 0.7980\n",
            "Epoch 69/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3761 - acc: 0.8704 - val_loss: 0.5889 - val_acc: 0.8121\n",
            "Epoch 70/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.3691 - acc: 0.8741 - val_loss: 0.5936 - val_acc: 0.8033\n",
            "Epoch 71/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3659 - acc: 0.8749 - val_loss: 0.5761 - val_acc: 0.8148\n",
            "Epoch 72/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3622 - acc: 0.8770 - val_loss: 0.5701 - val_acc: 0.8131\n",
            "Epoch 73/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3571 - acc: 0.8782 - val_loss: 0.5832 - val_acc: 0.8079\n",
            "Epoch 74/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3590 - acc: 0.8774 - val_loss: 0.5770 - val_acc: 0.8105\n",
            "Epoch 75/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3537 - acc: 0.8790 - val_loss: 0.5650 - val_acc: 0.8076\n",
            "Epoch 76/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3390 - acc: 0.8825 - val_loss: 0.5751 - val_acc: 0.8090\n",
            "Epoch 77/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.3439 - acc: 0.8813 - val_loss: 0.5968 - val_acc: 0.8081\n",
            "Epoch 78/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3399 - acc: 0.8836 - val_loss: 0.5679 - val_acc: 0.8142\n",
            "Epoch 79/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3355 - acc: 0.8857 - val_loss: 0.5549 - val_acc: 0.8137\n",
            "Epoch 80/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3453 - acc: 0.8821 - val_loss: 0.6144 - val_acc: 0.8126\n",
            "Epoch 81/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.3357 - acc: 0.8841 - val_loss: 0.5829 - val_acc: 0.8127\n",
            "Epoch 82/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3241 - acc: 0.8894 - val_loss: 0.5474 - val_acc: 0.8257\n",
            "Epoch 83/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3361 - acc: 0.8854 - val_loss: 0.5963 - val_acc: 0.8117\n",
            "Epoch 84/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3262 - acc: 0.8879 - val_loss: 0.5757 - val_acc: 0.8129\n",
            "Epoch 85/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3248 - acc: 0.8890 - val_loss: 0.5689 - val_acc: 0.8137\n",
            "Epoch 86/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3179 - acc: 0.8905 - val_loss: 0.5510 - val_acc: 0.8207\n",
            "Epoch 87/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3151 - acc: 0.8914 - val_loss: 0.5715 - val_acc: 0.8129\n",
            "Epoch 88/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3228 - acc: 0.8899 - val_loss: 0.5639 - val_acc: 0.8202\n",
            "Epoch 89/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3151 - acc: 0.8927 - val_loss: 0.5832 - val_acc: 0.8170\n",
            "Epoch 90/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3179 - acc: 0.8902 - val_loss: 0.5907 - val_acc: 0.8125\n",
            "Epoch 91/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3139 - acc: 0.8921 - val_loss: 0.5554 - val_acc: 0.8251\n",
            "Epoch 92/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3144 - acc: 0.8926 - val_loss: 0.5803 - val_acc: 0.8193\n",
            "Epoch 93/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3060 - acc: 0.8971 - val_loss: 0.6250 - val_acc: 0.8074\n",
            "Epoch 94/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3060 - acc: 0.8957 - val_loss: 0.6003 - val_acc: 0.8115\n",
            "Epoch 95/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.3094 - acc: 0.8932 - val_loss: 0.5726 - val_acc: 0.8149\n",
            "Epoch 96/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.3005 - acc: 0.8951 - val_loss: 0.5930 - val_acc: 0.8130\n",
            "Epoch 97/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2955 - acc: 0.8980 - val_loss: 0.6269 - val_acc: 0.8092\n",
            "Epoch 98/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2913 - acc: 0.9004 - val_loss: 0.5710 - val_acc: 0.8158\n",
            "Epoch 99/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2871 - acc: 0.9024 - val_loss: 0.5487 - val_acc: 0.8227\n",
            "Epoch 100/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2708 - acc: 0.9077 - val_loss: 0.5535 - val_acc: 0.8253\n",
            "Epoch 101/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2740 - acc: 0.9051 - val_loss: 0.5453 - val_acc: 0.8282\n",
            "Epoch 102/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2742 - acc: 0.9067 - val_loss: 0.5640 - val_acc: 0.8216\n",
            "Epoch 103/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2720 - acc: 0.9072 - val_loss: 0.5328 - val_acc: 0.8305\n",
            "Epoch 104/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2681 - acc: 0.9087 - val_loss: 0.5488 - val_acc: 0.8239\n",
            "Epoch 105/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2698 - acc: 0.9072 - val_loss: 0.5478 - val_acc: 0.8296\n",
            "Epoch 106/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2686 - acc: 0.9073 - val_loss: 0.5368 - val_acc: 0.8279\n",
            "Epoch 107/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2681 - acc: 0.9096 - val_loss: 0.5473 - val_acc: 0.8280\n",
            "Epoch 108/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2679 - acc: 0.9059 - val_loss: 0.5506 - val_acc: 0.8274\n",
            "Epoch 109/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2629 - acc: 0.9096 - val_loss: 0.5731 - val_acc: 0.8262\n",
            "Epoch 110/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2661 - acc: 0.9083 - val_loss: 0.5585 - val_acc: 0.8251\n",
            "Epoch 111/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2568 - acc: 0.9111 - val_loss: 0.5636 - val_acc: 0.8309\n",
            "Epoch 112/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2608 - acc: 0.9096 - val_loss: 0.5506 - val_acc: 0.8282\n",
            "Epoch 113/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2609 - acc: 0.9119 - val_loss: 0.5555 - val_acc: 0.8294\n",
            "Epoch 114/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2643 - acc: 0.9103 - val_loss: 0.5543 - val_acc: 0.8288\n",
            "Epoch 115/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.2625 - acc: 0.9106 - val_loss: 0.5472 - val_acc: 0.8306\n",
            "Epoch 116/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2582 - acc: 0.9111 - val_loss: 0.5418 - val_acc: 0.8292\n",
            "Epoch 117/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2571 - acc: 0.9117 - val_loss: 0.5365 - val_acc: 0.8310\n",
            "Epoch 118/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2582 - acc: 0.9112 - val_loss: 0.5250 - val_acc: 0.8328\n",
            "Epoch 119/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2595 - acc: 0.9098 - val_loss: 0.5331 - val_acc: 0.8358\n",
            "Epoch 120/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2576 - acc: 0.9113 - val_loss: 0.5439 - val_acc: 0.8265\n",
            "Epoch 121/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2491 - acc: 0.9155 - val_loss: 0.5671 - val_acc: 0.8317\n",
            "Epoch 122/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2568 - acc: 0.9116 - val_loss: 0.5339 - val_acc: 0.8374\n",
            "Epoch 123/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2609 - acc: 0.9106 - val_loss: 0.5485 - val_acc: 0.8310\n",
            "Epoch 124/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2567 - acc: 0.9121 - val_loss: 0.5490 - val_acc: 0.8323\n",
            "Epoch 125/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2573 - acc: 0.9129 - val_loss: 0.5410 - val_acc: 0.8310\n",
            "Epoch 126/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2487 - acc: 0.9136 - val_loss: 0.5532 - val_acc: 0.8309\n",
            "Epoch 127/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2550 - acc: 0.9123 - val_loss: 0.5498 - val_acc: 0.8310\n",
            "Epoch 128/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2542 - acc: 0.9135 - val_loss: 0.5484 - val_acc: 0.8325\n",
            "Epoch 129/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2551 - acc: 0.9129 - val_loss: 0.5677 - val_acc: 0.8239\n",
            "Epoch 130/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2477 - acc: 0.9150 - val_loss: 0.5541 - val_acc: 0.8292\n",
            "Epoch 131/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2527 - acc: 0.9125 - val_loss: 0.5366 - val_acc: 0.8318\n",
            "Epoch 132/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2469 - acc: 0.9154 - val_loss: 0.5774 - val_acc: 0.8306\n",
            "Epoch 133/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2486 - acc: 0.9129 - val_loss: 0.5609 - val_acc: 0.8274\n",
            "Epoch 134/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.2466 - acc: 0.9168 - val_loss: 0.5514 - val_acc: 0.8253\n",
            "Epoch 135/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2456 - acc: 0.9150 - val_loss: 0.5493 - val_acc: 0.8351\n",
            "Epoch 136/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2420 - acc: 0.9171 - val_loss: 0.5297 - val_acc: 0.8330\n",
            "Epoch 137/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2442 - acc: 0.9167 - val_loss: 0.5587 - val_acc: 0.8289\n",
            "Epoch 138/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2497 - acc: 0.9136 - val_loss: 0.5542 - val_acc: 0.8325\n",
            "Epoch 139/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2421 - acc: 0.9180 - val_loss: 0.5508 - val_acc: 0.8338\n",
            "Epoch 140/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2424 - acc: 0.9151 - val_loss: 0.5196 - val_acc: 0.8394\n",
            "Epoch 141/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2406 - acc: 0.9172 - val_loss: 0.5748 - val_acc: 0.8264\n",
            "Epoch 142/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2374 - acc: 0.9164 - val_loss: 0.5243 - val_acc: 0.8365\n",
            "Epoch 143/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2402 - acc: 0.9179 - val_loss: 0.5668 - val_acc: 0.8309\n",
            "Epoch 144/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2477 - acc: 0.9157 - val_loss: 0.5543 - val_acc: 0.8376\n",
            "Epoch 145/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2409 - acc: 0.9172 - val_loss: 0.5521 - val_acc: 0.8355\n",
            "Epoch 146/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2439 - acc: 0.9147 - val_loss: 0.5778 - val_acc: 0.8291\n",
            "Epoch 147/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2427 - acc: 0.9186 - val_loss: 0.5515 - val_acc: 0.8352\n",
            "Epoch 148/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2463 - acc: 0.9157 - val_loss: 0.5405 - val_acc: 0.8301\n",
            "Epoch 149/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2468 - acc: 0.9160 - val_loss: 0.5736 - val_acc: 0.8275\n",
            "Epoch 150/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2481 - acc: 0.9153 - val_loss: 0.5787 - val_acc: 0.8247\n",
            "Epoch 151/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2401 - acc: 0.9163 - val_loss: 0.5351 - val_acc: 0.8377\n",
            "Epoch 152/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2402 - acc: 0.9172 - val_loss: 0.5352 - val_acc: 0.8342\n",
            "Epoch 153/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 79s - loss: 0.2413 - acc: 0.9152 - val_loss: 0.5309 - val_acc: 0.8391\n",
            "Epoch 154/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2407 - acc: 0.9176 - val_loss: 0.5748 - val_acc: 0.8305\n",
            "Epoch 155/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2421 - acc: 0.9168 - val_loss: 0.5533 - val_acc: 0.8303\n",
            "Epoch 156/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2427 - acc: 0.9178 - val_loss: 0.5525 - val_acc: 0.8348\n",
            "Epoch 157/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2472 - acc: 0.9149 - val_loss: 0.5404 - val_acc: 0.8346\n",
            "Epoch 158/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2451 - acc: 0.9163 - val_loss: 0.5479 - val_acc: 0.8315\n",
            "Epoch 159/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2425 - acc: 0.9159 - val_loss: 0.5548 - val_acc: 0.8326\n",
            "Epoch 160/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2447 - acc: 0.9162 - val_loss: 0.5595 - val_acc: 0.8370\n",
            "Epoch 161/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2420 - acc: 0.9157 - val_loss: 0.5717 - val_acc: 0.8331\n",
            "Epoch 162/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2371 - acc: 0.9184 - val_loss: 0.5204 - val_acc: 0.8390\n",
            "Epoch 163/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2371 - acc: 0.9185 - val_loss: 0.5588 - val_acc: 0.8358\n",
            "Epoch 164/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2390 - acc: 0.9195 - val_loss: 0.5468 - val_acc: 0.8390\n",
            "Epoch 165/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 77s - loss: 0.2375 - acc: 0.9171 - val_loss: 0.5212 - val_acc: 0.8370\n",
            "Epoch 166/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2445 - acc: 0.9141 - val_loss: 0.5621 - val_acc: 0.8300\n",
            "Epoch 167/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2403 - acc: 0.9184 - val_loss: 0.5516 - val_acc: 0.8302\n",
            "Epoch 168/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2401 - acc: 0.9195 - val_loss: 0.5419 - val_acc: 0.8366\n",
            "Epoch 169/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2422 - acc: 0.9161 - val_loss: 0.5554 - val_acc: 0.8313\n",
            "Epoch 170/1000\n",
            "Epoch 1/1000\n",
            "18/18 - 78s - loss: 0.2356 - acc: 0.9189 - val_loss: 0.5756 - val_acc: 0.8332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeezaVmeaCQ5",
        "colab_type": "code",
        "outputId": "f3d4a189-fe52-4601-daa3-5045eb9f1a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "plot_acc_and_loss(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAE/CAYAAADVKysfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7dc_vwCal_R",
        "colab_type": "code",
        "outputId": "81c20dfd-b95c-433d-c364-df5f419f2987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   9472        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 64)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 16, 16, 64)   4160        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 16, 16, 64)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 256)  16640       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 256)  16640       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16, 16, 256)  0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   16448       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 64)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   36928       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 256)  65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 256)  16640       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 16, 256)  0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   16448       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 64)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 256)  65792       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 256)  16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 256)  0           conv2d_12[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)    0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    32896       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 8, 8, 128)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147584      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 8, 8, 128)    512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 512)    131584      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 512)    66048       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 8, 8, 512)    0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 128)    65664       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 8, 8, 128)    0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 128)    147584      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 128)    512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 512)    262656      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 512)    66048       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 512)    0           conv2d_20[0][0]                  \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    65664       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 512)    262656      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 512)    66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 512)    0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 512)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 4, 4, 256)    131328      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 4, 4, 256)    0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 4, 4, 256)    590080      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 4, 4, 256)    1024        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 256)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 4, 4, 1024)   525312      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 4, 4, 1024)   263168      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 1024)   0           conv2d_28[0][0]                  \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 4, 4, 256)    262400      add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 4, 4, 256)    590080      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 4, 4, 256)    1024        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 256)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 4, 4, 1024)   1049600     add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 4, 4, 1024)   263168      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 1024)   0           conv2d_32[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 4, 4, 256)    262400      add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 4, 4, 256)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 4, 4, 256)    590080      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 4, 4, 256)    1024        conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 4, 4, 256)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 4, 4, 1024)   1049600     add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 4, 4, 1024)   263168      activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 1024)   0           conv2d_36[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 1024)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 2, 2, 512)    524800      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 2, 2, 512)    0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 2, 2, 512)    2359808     activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 2, 2, 512)    2048        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 2, 2, 512)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 2, 2, 2048)   2099200     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 2, 2, 2048)   0           conv2d_40[0][0]                  \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 2, 2, 512)    1049088     add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 2, 2, 512)    0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 2, 2, 512)    2359808     activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 2, 2, 512)    2048        conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 2, 2, 512)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 2, 2, 2048)   4196352     add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 2, 2, 2048)   0           conv2d_44[0][0]                  \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 2, 2, 512)    1049088     add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 2, 2, 512)    0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 2, 2, 512)    2359808     activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 2, 2, 512)    2048        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 2, 2, 512)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 2, 2, 2048)   4196352     add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 2, 2, 2048)   0           conv2d_48[0][0]                  \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2048)         4196352     global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 516)          1057284     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 516)          2064        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         529408      batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 1024)         4096        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           10250       batch_normalization_14[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 36,815,262\n",
            "Trainable params: 36,806,294\n",
            "Non-trainable params: 8,968\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tijkBOIx8JF",
        "colab_type": "code",
        "outputId": "63691a63-dc5a-4a2a-ef89-33412f3352ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp submission_expresnet.csv \"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'submission_expresnet.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyK5ywXnaCQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# try tta\n",
        "tta_epochs = 50\n",
        "tta_pred = tta(model, x_test.shape[0], tta_generator(x_test, batch_size=1000), batch_size=1000, epochs=tta_epochs)\n",
        "print(tta_pred.shape)\n",
        "print(tta_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um9JtlvraCQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tta_pred = np.argmax(tta_pred, axis=1)\n",
        "submission = pd.Series(tta_pred, name='label')\n",
        "submission.to_csv('tta_submission2.csv', header=True, index_label='id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ryrYz25cVjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp tta_submission.csv \"/content/drive/My Drive/Colab Notebooks/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5dvGDFPaCQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('model.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}