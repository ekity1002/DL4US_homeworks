{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson3 系列データで分類・予測させてみよう（RNN, LSTM）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "RNNを用いてさらに高精度なECG5000の分類器を作ってみましょう。\n",
    "\n",
    "ネットワークの形などは特に制限を設けませんし、今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません。\n",
    "\n",
    "上位者はリーダーボードに掲載させていただきます。（評価はaccuracyによって行います。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目標値\n",
    "Accuracy 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ルール\n",
    "- 訓練データは`x_train`, `y_train`, テストデータは`x_test`で与えられます.\n",
    "- 予測ラベルは **one_hot表現ではなく0~4のクラスラベル** で表してください.\n",
    "- 下のセルで指定されているx_train, y_train以外の学習データは使わないでください."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価について\n",
    "\n",
    "- テストデータ(x_test)に対する予測ラベルをcsvファイルで提出してください.\n",
    "- ファイル名はsubmission.csvとしてください.\n",
    "- 予測ラベルのy_testに対する精度 (F値)で評価します.\n",
    "- 毎日24時にテストデータの一部に対する精度でLeader Boardを更新します.\n",
    "- 最終的な評価はテストデータ全体に対する精度でおこないます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルコード\n",
    "**次のセルで指定されているx_train, y_trainのみを使って学習させてください.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, add, concatenate, Dense, Activation, SimpleRNN, LSTM, CuDNNLSTM, Bidirectional, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "SEED = 9999\n",
    "AUG_NUM = [100,100,100]\n",
    "\n",
    "def load_dataset():\n",
    "    # 学習データ\n",
    "    x_train = np.load('/root/userspace/public/lesson3/data/x_train.npy')\n",
    "    y_train = np.load('/root/userspace/public/lesson3/data/y_train.npy')\n",
    "    y_train = to_categorical(y_train[:, np.newaxis], num_classes = 5)\n",
    "    \n",
    "    # テストデータ\n",
    "    x_test = np.load('/root/userspace/public/lesson3/data/x_test.npy')\n",
    "    \n",
    "    #水増し \n",
    "#     aug=True\n",
    "#     if aug:\n",
    "#         x_train, y_train = augwave(x_train,y_train,AUG_NUM) \n",
    "\n",
    "    #標準化\n",
    "    std=True\n",
    "    if std==True:\n",
    "        scl = StandardScaler()\n",
    "        scl.fit(x_train[:,:,0])\n",
    "        x_train = scl.transform(x_train[:,:,0]).reshape(x_train.shape)\n",
    "        x_test = scl.transform(x_test[:,:,0]).reshape(x_test.shape)\n",
    "        print(\"std: \", x_train.shape, x_test.shape)\n",
    "        \n",
    "    # roll\n",
    "    \n",
    "    # min-max scl はよくない気がする。最大値がわからないので    \n",
    "    return x_train, x_test, y_train\n",
    "\n",
    "def augwave(x,y,n_list):\n",
    "    \"\"\"2,3,4のラベルにノイズを加えて水増しする\"\"\"\n",
    "    aug_labels = [2,3,4]\n",
    "    for label, n in zip(aug_labels, n_list):\n",
    "        # 増やすラベルの波形のインデックス\n",
    "        label_idx = np.where(np.argmax(y, axis=1) == label)[0]\n",
    "\n",
    "        # ノイズを加える波形をn個選ぶ\n",
    "        indices = np.random.choice(label_idx, n, replace=True)\n",
    "        #ランダムにノイズ追加\n",
    "        tmp_x = x[indices].copy()\n",
    "        tmp_x += (np.random.rand(*tmp_x.shape)-0.5)*0.8 #\n",
    "        tmp_y = y[indices].copy()\n",
    "\n",
    "        # 追加\n",
    "        x = np.vstack((x,tmp_x))\n",
    "        y = np.vstack((y, tmp_y))\n",
    "    print('augx: ',x.shape)\n",
    "    print('augy: ',y.shape)\n",
    "    return x, y\n",
    "\n",
    "def fft(x, n_freq=100):\n",
    "    tmp = x[:,:,0].copy()\n",
    "    # 横にくっつける\n",
    "    for _ in range(n_freq-1):\n",
    "        tmp = np.hstack((tmp, x[:,:,0]))\n",
    "    #plt.plot(tmp)\n",
    "\n",
    "    # 高速フーリエ変換(FFT)\n",
    "    N = x.shape[1]*n_freq\n",
    "    #print(tmp.shape, N)\n",
    "    F = np.fft.fft(tmp)\n",
    "    # FFT結果（複素数）を絶対値に変換\n",
    "    F_abs = np.abs(F)\n",
    "    \n",
    "    # 振幅を元に信号に揃える\n",
    "    F_abs_amp = F_abs / N * 2 # 交流成分はデータ数で割って2倍する\n",
    "    # グラフ表示\n",
    "    #plt.plot(F_abs_amp[:, :int(N/2)+1])\n",
    "    return F_abs_amp[:, :int(N/2)+1]\n",
    "\n",
    "x_train, x_test, y_train = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fftx_train = fft(x_train)\n",
    "fftx_test = fft(x_test)\n",
    "fftx_train = fftx_train.reshape(*fftx_train.shape, 1)\n",
    "fftx_test = fftx_test.reshape(*fftx_test.shape, 1)\n",
    "# amp = fft(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape)\n",
    "print(fftx_train.shape, fftx_test.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "print(pd.Series(np.argmax(y_train, axis=1)).value_counts())\n",
    "sns.countplot(pd.Series(np.argmax(y_train, axis=1)), ax=ax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\"\"\"\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "def build_model():\n",
    "    hid_dim = 32\n",
    "    input1 = Input(x_train.shape[1:])\n",
    "    input2 = Input(fftx_train.shape[1:])\n",
    "    \n",
    "    def cnn_lstm_layer(inputs, filter_size, kernel_size, layer_num=3, ):\n",
    "        for i in range(layer_num):\n",
    "            if i == 0:\n",
    "                x = Conv1D(filter_size, kernel_size, padding='same', kernel_initializer='he_normal', input_shape=x_train.shape[1:])(inputs)\n",
    "            else:\n",
    "                x = Conv1D(filter_size, kernel_size, padding='same', kernel_initializer='he_normal')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "        x = Bidirectional(CuDNNLSTM(hid_dim))(x)\n",
    "        return x\n",
    "    x1 = cnn_lstm_layer(input1, 32, 40)\n",
    "    x2 = cnn_lstm_layer(input2, 32, 40)\n",
    "    x = concatenate([x1, x2])\n",
    "    out = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "    model = Model(inputs=[input1, input2], outputs=out)\n",
    "    model.compile(loss=f1_loss, optimizer=Adam(), metrics=['accuracy', f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練に使用しないデータ x_val, y_val\n",
    "x_tra, x_val, y_tra, y_val = train_test_split(x_train, y_train, test_size=0.33, random_state=SEED, stratify=y_train)\n",
    "fftx_tra, fftx_val, ffty_tra, ffty_val = train_test_split(fftx_train, y_train, test_size=0.33, random_state=SEED, stratify=y_train)\n",
    "#x_tra, y_tra = augwave(x_tra, y_tra, AUG_NUM)\n",
    "print(x_tra.shape, y_tra.shape)\n",
    "print(fftx_tra.shape, fftx_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.argmax(y_tra, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "modelpath = '/root/userspace/cnn_lstm_model.hdf5'\n",
    "callbacks = [\n",
    "        EarlyStopping(monitor='val_acc', patience=8, mode='max',min_delta=0.001),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=4, mode='max', min_delta=0.001),\n",
    "        ModelCheckpoint(filepath=modelpath, monitor='val_acc', save_best_only=True, mode='max')\n",
    "    ]\n",
    "history = model.fit([x_tra, fftx_tra], y_tra, epochs=1000, batch_size=128, verbose=2,\n",
    "                    validation_data=([x_val,fftx_val], y_val), callbacks=callbacks)\n",
    "# model.fit_generator(my_generator(x_tra, y_tra, batch_size),\n",
    "#                     steps_per_epoch=len(tr_X)//batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     validation_data=my_generator(val_X, val_y, batch_size),\n",
    "#                     validation_steps=len(val_X)//batch_size,\n",
    "#                     validation_split=\n",
    "#                     callbacks=callbacks,\n",
    "#                     verbose=2)\n",
    "\n",
    "# y_pred = np.argmax(model.predict(x_test), 1)\n",
    "\n",
    "# submission = pd.Series(y_pred, name='label')\n",
    "# submission.to_csv('/root/userspace/submission.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度確認\n",
    "y_pred = model.predict([x_val, fftx_val])\n",
    "print('f1_macro: ', f1_score(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1), average='macro'))\n",
    "print('acc: ', accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(model.evaluate([x_train, fftx_train], y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列\n",
    "cm = confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# cv\n",
    "def run_cv(train, test, target, params={}):\n",
    "    N = 5\n",
    "    kf = StratifiedKFold(n_splits=N, random_state=SEED, shuffle=True)\n",
    "    fold_splits = kf.split(train, target.argmax(axis=1))\n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "    results = np.zeros((test.shape[0], N+1))\n",
    "    i = 0\n",
    "    \n",
    "    for tr_idx, val_idx in fold_splits:\n",
    "        print(f'Start fold {i+1}/{N}')\n",
    "        tr_X, val_X = [train[tr_idx, :], fftx_train[tr_idx, :]], [train[val_idx, :], fftx_train[val_idx, :]]\n",
    "        tr_y, val_y = target[tr_idx, :], target[val_idx, :]\n",
    "        print(tr_X[0].shape, tr_X[1].shape)\n",
    "        params['modelpath'] = f'/root/userspace/cnn_lstm_fft_cv{i}_model.hdf5'\n",
    "\n",
    "        val_acc, val_f1, test_pred = run_model(tr_X, tr_y, val_X, val_y, test, params)\n",
    "        acc_scores.append(val_acc)\n",
    "        f1_scores.append(val_f1)\n",
    "        results[:, i] = test_pred\n",
    "        i+=1\n",
    "    print('mean acc: ', np.mean(acc_scores))\n",
    "    print('mean F1 : ', np.mean(f1_scores))\n",
    "    return results\n",
    "\n",
    "# モデル予測実行\n",
    "def run_model(tr_X, tr_y, val_X, val_y, test, params):\n",
    "    print('Train model')\n",
    "    batch_size=params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    modelpath = params['modelpath']\n",
    "\n",
    "    # 訓練データ水増し\n",
    "    #tr_X, tr_y = augwave(tr_X, tr_y, AUG_NUM)\n",
    "    print(pd.Series(np.argmax(tr_y, axis=1)).value_counts())\n",
    "    \n",
    "    model = build_model()    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_acc', patience=8, mode='max', min_delta=0.001),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=4, mode='max', min_delta=0.001),\n",
    "        ModelCheckpoint(filepath=modelpath, monitor='val_acc', save_best_only=True, mode='max')\n",
    "    ]\n",
    "    history = model.fit(tr_X, tr_y, epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=2, validation_data=(val_X, val_y),\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    print('Pred 1/2')\n",
    "    \n",
    "    #load_best model and eval\n",
    "    model = load_model(modelpath,  custom_objects={'f1_loss': f1_loss, 'f1': f1})\n",
    "    \n",
    "    # cmx\n",
    "    y_pred = model.predict(val_X)\n",
    "    cm = confusion_matrix(np.argmax(val_y, axis=1), np.argmax(y_pred, axis=1))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues') \n",
    "    plt.show()\n",
    "        \n",
    "    tr_loss, tr_acc, tr_f1 = model.evaluate(tr_X, tr_y)\n",
    "    val_loss, val_acc, val_f1 = model.evaluate(val_X, val_y)\n",
    "    print(f'[Train] acc:{tr_acc}  loss:{tr_loss}  f1:{tr_f1}')\n",
    "    print(f'[Val]   acc:{val_acc} loss:{val_loss} f1:{val_f1}')\n",
    "    \n",
    "    print('Pred 2/2')\n",
    "    pred_test = np.argmax(model.predict([test, fftx_test]), axis=1)\n",
    "    return val_acc, val_f1, pred_test\n",
    "\n",
    "params = {'batch_size':128,\n",
    "          'epochs':1000,}\n",
    "results = run_cv(x_train, x_test, y_train, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(results.shape)\n",
    "submission = pd.DataFrame(results, dtype=int)\n",
    "submission = submission.apply(lambda x: np.argmax(x.value_counts()), axis=1)\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.name = 'label'\n",
    "submission.to_csv('/root/userspace/cnn_lstm_fft.csv', header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
